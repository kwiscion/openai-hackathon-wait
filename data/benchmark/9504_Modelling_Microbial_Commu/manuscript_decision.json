{
    "decision": "major revisions",
    "rationale": "The manuscript introduces a permutation-equivariant graph-neural approach to predict microbial community steady-state composition directly from genomic content\u2014a timely and potentially high-impact contribution. Reviewers concur that the methodological idea is sound, the experimental campaign is broad (two wet-lab data sets plus a simulator), and the authors plan to release code and data, all of which argue in favour of eventual publication. \n\nHowever, the current version over-states its conclusions and omits several analyses that are necessary for scientific robustness and for readers to judge the true merit of the work. Chief shortcomings include: (i) lack of comparison with accepted mechanistic baselines (e.g., gLV, genome-scale metabolic models) and with established permutation-equivariant set models (e.g., DeepSets); (ii) evaluation protocols and metrics that do not respect the compositional nature of microbiome data and allow possible data leakage, thereby inflating reported accuracy; (iii) insufficient analysis of uncertainty and of the biological meaning of learned features, undermining interpretability claims; (iv) coarse genomic feature engineering and a simulator that is too abstract to justify biological conclusions; and (v) overstated claims of generalization to unseen species given modest performance and potential overlap between train and test genomes.  Addressing these issues will require additional experiments, re-analysis of data, and clearer framing of limitations.  Hence major revisions are essential before the paper can be considered publishable.",
    "key_strengths": "1. Targets an important open problem in microbial ecology: predicting community composition without time-series data.\n2. Introduces a GNN formulation that naturally enforces permutation equivariance and shows promising accuracy over naive MLP baselines.\n3. Experiments span two independent laboratory data sets and a configurable simulator, exploring multiple generalization axes.\n4. Commitment to public release of code, processed data, and simulator promotes reproducibility and community uptake.\n5. Manuscript is generally well organised and readable; supplementary material is extensive.",
    "key_weaknesses": "1. Missing or weak baselines: no mechanistic gLV comparison; no DeepSets/SetTransformer or other permutation-equivariant non-GNN baselines; MLP baselines may be unfairly configured.\n2. Evaluation concerns: centred R\u00b2 ignores compositional constraints; no Aitchison or CLR-based metrics; replicate splitting/averaging may leak information; output normalisation choices not justified; no predictive uncertainty reported.\n3. Generalization claims potentially inflated: species hold-out splits use substitute genomes; unclear whether genome similarity causes leakage; unseen-species accuracy remains low for key strains.\n4. Biological realism gaps: environmental factors omitted; genomic features reduced to binary gene presence, discarding copy number, operon context, or pathway structure; simulator lacks higher-order interactions and external resources.\n5. Interpretability deficit: no systematic feature attribution, pathway enrichment, or correspondence between learned embeddings and ecological mechanisms.\n6. Scalability not demonstrated beyond fully-connected graphs of ~30 nodes; computational cost for 100+ species unclear.\n7. Minor: inconsistent references, limited detail on annotation pipeline parameters, and fragmented discussion of limitations.",
    "specific_revisions": "1. Add mechanistic and set-based baselines:\n   a) Fit a standard gLV model (regularised if necessary) on time-series subsets and report steady-state prediction accuracy.\n   b) Implement DeepSets or SetTransformer with comparable parameter count and report results on all tasks.\n2. Re-design evaluation:\n   a) Use compositional metrics (CLR-transformed RMSE, Aitchison distance) in addition to centred R\u00b2.\n   b) Ensure strict separation of genomes between train/val/test in unseen-species experiments; document ANI thresholds and justify any substitute genomes.\n   c) Report prediction intervals or Bayesian uncertainty estimates (e.g., MC-dropout) for key tasks.\n3. Provide interpretability analyses:\n   a) Conduct feature attribution (e.g., integrated gradients) to identify influential KOs/genes; map top features to pathways and discuss biological plausibility.\n   b) Explore embedding similarity versus phylogenetic distance.\n4. Improve genomic feature engineering or justify limitations: consider gene copy number, COG/KEGG pathway aggregates, or provide ablation quantifying effect of current binarisation.\n5. Extend simulator or temper conclusions: either include higher-order interaction terms and resource gradients, or clearly state that current simulator cannot address such factors and limit claims accordingly.\n6. Demonstrate computational scalability: benchmark runtime/memory for 50-, 100-, and 200-species communities; discuss sparse graph or sampling strategies.\n7. Add a responsible-use statement addressing potential dual-use of community-design predictions.\n8. Correct reference formatting and supply full genome annotation details in the appendix.",
    "priority_issues": [
        "Add and fairly evaluate mechanistic (gLV) and DeepSets/SetTransformer baselines",
        "Adopt compositional metrics and tighten data-splitting to avoid leakage",
        "Perform interpretability and uncertainty analyses",
        "Clarify or improve genomic feature representation",
        "Provide responsible-use / dual-use discussion"
    ],
    "ethical_considerations": "No direct human or sensitive data are used.  Nevertheless, enhanced predictive power for microbial consortia could facilitate malicious engineering.  Authors should include a brief statement on responsible use, limitations of accuracy, and the need for biosafety oversight, as recommended by reviewers.  No other ethical concerns were identified (no human subjects, no animal work, genomes are public)."
}