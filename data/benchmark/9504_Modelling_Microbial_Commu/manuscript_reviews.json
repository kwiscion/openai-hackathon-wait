[
    {
        "strengths": "1. Ambitious scope: tackling steady-state abundance prediction directly from genomic features is novel and, if validated, would remove the need for labor-intensive time-series experiments.\n2. Appropriate inductive bias: formulating communities as fully-connected graphs and using permutation-equivariant GNNs is methodologically sound and clearly outperforms MLP baselines wrt invariance.\n3. Comprehensive experimental suite: two independent wet-lab datasets plus an open-source simulator help probe several axes (community size, unseen species, interaction density, sample size).\n4. Thorough ablation/controls: comparisons against MLP with/without shuffling, multiple GNN variants, and boot-strapped confidence intervals give a transparent view of performance.\n5. Reproducibility: code, genomes, and simulator promised to be released; detailed appendix lists hyper-parameters and strains.\n6. Opens path for biological interpretation: message-passing layers could in principle highlight gene sets mediating interactions; authors note this explicitly and provide some qualitative hypotheses (butyrate producer AC, keystone BT).",
        "weaknesses": "1. Ecological oversimplifications:\n   \u2022 Environment ignored\u2014even in gnotobiotic reactors pH, carbon source, redox, phage cocktail differ between experiments; datasets used here were collected in rich medium but not identical across species. Claim that ignoring abiotic factors is \"justified\" (Sec. 2.1) is unconvincing.\n   \u2022 Steady-state assumption: in both datasets cultures were sampled after 48\u2013120 h batch passages; many mixes are not at ecological equilibrium (see oscillations in Friedman mono-cultures). Using day-5 snapshot as ground truth may bias training.\n2. Baseline selection:\n   \u2022 The gLV comparator is missing. Authors argue they learn dynamics implicitly, yet do not fit gLV on the same data (e.g., UMA fit using analytic steady-state solution) to show superiority. Without this, we cannot gauge the advantage over mechanistic pairwise models.\n   \u2022 Higher-order interactions are invoked as motivation but never explicitly tested; simulation generator is strictly pairwise, so GNN benefit over gLV for H.O.I. remains speculative.\n3. Generalization claims:\n   \u2022 Leave-one-species-out splits are small (often <10 test communities) and confounded by genome substitution (Pch) or strain-level mis-matches; R\u00b2 therefore hard to interpret.\n   \u2022 For larger communities authors train on \u226422 species and test on 23\u201326. These are still within training size range in Baranwal set; accuracy drops dramatically (R\u00b2\u22480.26). Claim of \"marginal\" generalization feels overstated.\n4. Simulator realism:\n   \u2022 Gene vectors are random binary encodings of gLV parameters; therefore mapping genome\u2192parameters is trivial and linear, favouring any model. No metabolic stoichiometry, gene regulation, or phylogenetic relatedness is represented.\n   \u2022 Interaction matrix drawn i.i.d. Laplace; real microbial networks are sparse and structured (e.g., block modular). Conclusions about sample size scaling may not transfer.\n5. Interpretability not delivered:\n   \u2022 No attempt to trace which KO groups contribute to predictions (e.g., saliency, attention weights). Thus the promise of \"testable hypotheses\" is not fulfilled.\n6. Statistical concerns:\n   \u2022 Using sigmoid on each node independently violates the simplex constraint; predictions can sum to >1 and are then renormalised(?)\u2014not specified.\n   \u2022 R\u00b2 computed after centring per community (Sec.2.2) inflates performance relative to absolute error.\n7. Writing/organisation:\n   \u2022 Several key choices relegated to appendix (replicate averaging, renormalisation). Main text should state them.\n   \u2022 Figures 3\u20136 hard to read; axes lack units and colours overlap.\n8. No ethical discussion of mis-use (e.g., designing pathogenic consortia).",
        "comments": "Major points to address before publication:\n1. Include mechanistic baselines.\n   \u2022 Fit standard gLV (pairwise) and, if feasible, extended Hildebrand cubic gLV, on both datasets using steady-state algebraic fitting (e.g., Koyama 2022 method). Report same splits.\n   \u2022 Compare on simulated data with known ground truth to verify whether GNN provides benefit when true interactions are high-order.\n2. Re-evaluate ecological assumptions.\n   \u2022 Show OD/time-series plots to justify equilibrium; alternatively, discuss non-equilibrium noise and train models to predict last-day snapshot instead of true steady state.\n   \u2022 Add environmental covariates (media, dilution factor) as global graph features and test improvement.\n3. Strengthen generalization analysis.\n   \u2022 Provide table of number of test communities per held-out species and confidence intervals.\n   \u2022 Perform phylogenetically stratified splits (e.g., leave-one-genus-out) to demonstrate broader extrapolation.\n4. Improve simulator.\n   \u2022 Encode genes via metabolic pathways: draw sparse stoichiometric matrices, compute flux-coupled Fitz gLV parameters, then one-hot KOs. Add higher-order (Michaelis-Menten) terms so that gLV is miss-specified.\n5. Interpretability.\n   \u2022 Run Integrated Gradients or GNNExplainer on top-performing models; highlight KO groups driving AC and BT effects and relate to known lactate utilisation operons or polysaccharide utilisation loci.\n6. Clarify output normalisation and evaluation metrics; report MAE in addition to centred R\u00b2.\n7. Minor: fix many citation identifiers (page-xx) artifacts; move large supplementary tables to GitHub.\n\nAfter these revisions, the paper would represent a solid contribution at the interface of microbial ecology and machine learning.",
        "rating": "fair",
        "confidence": "confident",
        "ethical_concerns": "None beyond standard data-use; however, authors may discuss dual-use implications of predictive models for microbiome manipulation."
    },
    {
        "strengths": "1. Timely problem framing \u2013 modelling microbial communities directly from genomes is a compelling alternative to gLV fitting on growth curves, and the paper is among the first to tackle it with neural\u2010set architectures.\n2. Clear justification of permutation-equivariant inductive bias and a principled choice of message\u2013passing GNNs; the performance gap to na\u00efve MLPs corroborates the hypothesis.\n3. Two public wet-lab data sets plus a configurable gLV simulator provide a useful test-bed; code release is promised.\n4. Experiments probe three axes of generalisation (new communities, larger communities, unseen species) and touch on scalability up to 200 species in silico.\n5. Paper is well-written, with an explicit reproducibility statement and extensive appendix.\n",
        "weaknesses": "1. Graph construction & update rules:   \n   \u2022 All graphs are treated as fully connected with uniform, feature-less edges. This removes any benefit of exploiting sparsity, ecological priors, or learned attention.   \n   \u2022 Edge features (e.g. co-occurrence, phylogeny or k-mer similarity) and edge-wise attention were not explored; GATv2 was rejected after minimal tuning and on tiny graphs.   \n2. Baselines & capacity matching:   \n   \u2022 The MLP baseline is arguably unfair \u2013 using a fixed species ordering allows memorisation; the shuffled variant has massively fewer effective parameters per species index.   \n   \u2022 No comparison to permutation-equivariant set functions such as DeepSets, Set Transformers, or Pooling-by-Multihead-Attention, which would isolate the contribution of graph connectivity vs. set equivariance.   \n   \u2022 Parameter counts are not reported, so capacity parity across models is unclear.   \n3. Inductive-bias analysis:   \n   \u2022 The claim that permutation-equivariance is the key driver is only supported by the shuffle ablation on one data set. A more rigorous study (e.g. DeepSets vs. GraphSAGE with identical hidden width) is missing.   \n4. Hyper-parameter search & training details:   \n   \u2022 Search space is very narrow (\u22643 layers, mean aggregation only, no dropout/weight-decay search). This may bias results in favour of the hand-picked models.   \n   \u2022 Validation loss curves, variance over seeds, and early-stopping criteria are not shown \u2013 limiting insight into stability.   \n5. Evaluation protocol:   \n   \u2022 Replicate measurements are averaged before splitting, risking information leakage (different replicates from the same community can enter train and test).   \n   \u2022 R\u00b2 is averaged over bootstrapped communities but compositional closure (sum=1) is ignored; a Dirichlet-aware loss or CLR-transformed R\u00b2 would be more appropriate.   \n   \u2022 The softmax vs. sigmoid choice is waved away, yet predictions should satisfy the simplex constraint.   \n6. Scalability claims:   \n   \u2022 200-species experiment is simulation-only; wall-clock time, GPU memory, and message complexity (O(N\u00b2)) are not reported.   \n   \u2022 In real applications hundreds to thousands of species are common; fully-connected O(N\u00b2) message passing is unlikely to scale.   \n7. Biological realism:   \n   \u2022 The simulated genome encoder (7-bit binary quantisation of parameters) is far removed from real pangenome structure, so conclusions about generalisation to unseen genomes are tentative.   \n8. Limited interpretability:   \n   \u2022 No attempt is made to extract learnt interaction motifs or to relate message weights to biological pathways; this weakens the usefulness for microbiologists.   \n",
        "comments": "Detailed feedback organised along requested axes:\n\n1. Architecture choices\n   \u2022 Consider sparse or adaptive graphs: an attention-based edge predictor conditioned on pairwise genome similarity could reduce O(N\u00b2) cost and may improve biological plausibility.\n   \u2022 Try incorporating edge attributes (e.g. ANI, metabolic overlap). Message passing with edge-conditioned filters (EGNN, DGN) or GAT with learned attention could capture directionality absent in mean aggregation.\n   \u2022 Report parameter counts and FLOPs so readers can judge whether gains stem from architecture or capacity.\n\n2. Inductive bias analysis\n   \u2022 Re-run experiments with DeepSets and Set Transformer baselines that share the same hidden dimension as GraphSAGE; this will isolate the contribution of connectivity vs. equivariance.\n   \u2022 A controlled synthetic task where only higher-order (\u22653) interactions matter would clarify whether message passing (k-hop context) rather than simple pooling drives the advantage.\n\n3. Hyper-parameter search & baselines\n   \u2022 Expand the search to include depth, width, dropout, weight decay, residual connections, and different aggregators (max, attention, Set2Set).\n   \u2022 For fairness, ensure all models are trained with identical optimisation schedules and tuned on the same validation folds.\n   \u2022 Publish a table with #parameters and best validation MSE for every model.\n\n4. Training stability & evaluation\n   \u2022 Provide learning curves with mean\u00b1std over seeds; discuss any divergence or overfitting symptoms.\n   \u2022 Split replicates at the sample level before averaging to prevent leakage, or fit a hierarchical model that treats replicate noise explicitly.\n   \u2022 Use a compositional evaluation metric (Aitchison distance or CLR-R\u00b2) and constrain outputs with softmax; otherwise predictions can violate the simplex.\n\n5. Scalability\n   \u2022 Add complexity analysis: memory ~ O(N d + |E| h) and time per epoch ~ O(|E| h\u00b2). At N=200 with full connectivity |E|\u224840k; discuss how far this can grow and whether GraphSAGE sampling or linear transformers for sets might help.\n   \u2022 Report training/inference time for the 200-species run and compare to a sparse k-NN graph.\n\n6. Biological grounding & interpretability\n   \u2022 Demonstrate that learned node embeddings correlate with known traits (e.g. butyrate pathway genes) or that attention weights recover known facilitation/inhibition pairs.\n   \u2022 Replace the binary genome vector with k-mer TFIDF or KO abundance and test whether richer inputs boost accuracy.\n\nMinor issues\n   \u2022 Some results (e.g. GAT/GCNII) are relegated to appendix without specification of tuning; clarify whether they were capacity matched.\n   \u2022 Table/figure numbering in the PDF is inconsistent; several references appear as placeholders (#page-x-y).\n\nOverall, the work is promising but requires stronger baselines, cleaner evaluation, and deeper analysis to substantiate the methodological claims. Addressing the points above would significantly strengthen the contribution.\n",
        "rating": "fair",
        "confidence": "uncertain",
        "ethical_concerns": "No direct ethical issues were detected. Data comes from publicly available microbial experiments. Privacy or dual-use concerns appear minimal."
    },
    {
        "strengths": "1. Ambitious attempt to connect whole-genome content with emergent community composition, moving beyond 16S or taxon ID to functional features.\n2. Proper choice of permutation\u2013equivariant architecture (MPGNN / GraphSAGE) and careful comparison with MLP baselines.\n3. Demonstrates three kinds of generalisation (new community structure, bigger communities, unseen taxa) that previous gLV and RNN work could not.\n4. Makes all code, processed data and simulator public; simulator could become a benchmark for many-species interaction modelling.\n5. Paper is well written, with extensive supplemental analyses and ablation studies.",
        "weaknesses": "(Genome feature handling)\n\u2022 KO presence/absence vectors are generated once per species, but the manuscript does not describe: (i) which annotation release of PGAP or KAAS was used, (ii) whether assemblies were re-annotated for consistency, or (iii) how multi-copy KOs were down-sampled (first copy? majority rule?).\n\u2022 Binary encoding discards gene copy number, allelic variation, operon structure and plasmid presence, all of which often modulate interaction strength (e.g. type-VI systems, antimicrobial cassettes). No justification is given that a 0/1 vector is sufficient; copy-number (\n\u22652) genes are silently collapsed.\n\u2022 Substitute genomes (P. chlororaphis for P. aurantiaca, B. caccae alternate strain, etc.) create feature noise that is later blamed for poor predictions. Yet the paper neither quantifies the Hamming distance between true vs proxy genomes nor re-runs models with a random subset of KOs to show robustness.\n\n(Potential confounding)\n\u2022 Models may learn phylogenetic similarity instead of mechanistic genes. No control is attempted (e.g. Mantel test of KO distance vs prediction error, or training on phylogeny-shuffled KO matrices).\n\u2022 Fully-connected graph and single message-passing depth forces every species to \u2018see\u2019 every other species; this makes it difficult to disentangle whether the node embedding or the edge network carries the predictive signal.\n\n(Data quality / reproducibility)\n\u2022 Draft vs complete genome status, assembly version and accession numbers are hidden in Table S4 but not referenced in main text; reproducibility would be easier with a Git repo containing raw FASTA and annotation scripts.\n\u2022 KO columns present in all genomes are dropped, which hampers downstream interpretability (house-keeping genes can still drive absolute growth rate differences). This decision is undocumented in the Methods.\n\n(Biological interpretability)\n\u2022 No feature-importance or saliency analysis is provided. Which KOs drive high abundance in community? Are these central carbon metabolism genes or niche-specific pathways? Without this, biological insight is limited.\n\u2022 The simulator encodes \u00b5, K and a_ij directly into artificial \u2018genes\u2019, but real genomes constrain interaction coefficients through metabolic flux, not inner-products; hence conclusions drawn from simulation (e.g., keystone transferability) may not carry over.\n\n(Experimental scope)\n\u2022 Real datasets are tiny (\u226493 and 459 samples) compared with 6.4k-dimensional KO vectors. Severe over-parameterisation risk is not addressed (no sparsity regulariser, no statistical power discussion).\n\u2022 Evaluation metric is per-species R\u00b2 after centring by community mean; this hides compositional coupling and forces sum<1 predictions.\n",
        "comments": "Detailed comments by topic requested in the prompt:\n1. Genome processing pipeline\n   \u2022 Please state the exact PGAP/Prodigal/KAAS versions, command line flags, and date of KEGG release. Mixed annotation sources (NCBI vs ATCC) routinely differ by 3\u20135 % in KO calls.\n   \u2022 Describe how draft assemblies with many contigs were handled. Did you remove frameshifted CDS features? Were plasmids included?\n   \u2022 When a strain genome was missing and a type-strain substitute was chosen, report ANI values or KO vector Jaccard similarity so readers can judge the introduced noise.\n2. Biological relevance of binary KO encoding\n   \u2022 Show a univariate regression of copy-number (e.g., ribosomal RNA operons, vitamin B12 pathway gene dosage) vs observed monoculture \u00b5 to demonstrate whether copy-number matters. If yes, augment the feature tensor with log-copy-number instead of 0/1.\n   \u2022 Consider collapsing KOs into pathway-level completeness scores or running CarveMe/ModelSEED to yield GEM-derived reaction sets; these are far more mechanistic and easier to interpret than sparse 6k-dimensional bit vectors.\n3. Data quality impact of substitutes\n   \u2022 Re-train the model after masking the 10 % most uncertain KOs in the proxy genomes; compare performance to original. This will quantify sensitivity to mis-annotation.\n   \u2022 At a minimum, add a column in Fig. 6 indicating whether a species uses a substitute genome.\n4. Phylogenetic confounders\n   \u2022 Compute a distance-based redundancy analysis: KO distance vs phylogenetic distance vs learned embedding distance. If r\u00b2 \u2248 1 the model is mainly capturing taxonomy.\n   \u2022 Alternatively, include a \u2018phylogeny only\u2019 baseline where node embeddings are 16-S distances; if that achieves similar R\u00b2 the KO features are superfluous.\n5. Reproducibility\n   \u2022 Provide a Snakemake or Nextflow pipeline that downloads each accession, runs the chosen annotation tool and produces the KO matrix. Without this, results cannot be replicated once KEGG or PGAP updates.\n   \u2022 Deposit the exact KO presence/absence matrix used for all experiments (after column filtering) in the repo so others can cross-reference.\n\nOther suggestions\n\u2022 Because sums of predicted abundances need to equal 1, apply a final Dirichlet layer or normalise logits across nodes to enforce compositionality.\n\u2022 Report per-community Bray\u2013Curtis or Aitchison distances, which are standard in microbiome studies.\n\u2022 Discuss ethical aspects of using genome data: any dual-use concerns (pathogen engineering) are minimal here.\n\nOverall, the machine-learning contribution is solid, but the genomic feature engineering lacks rigour and threatens biological credibility. Addressing the points above would substantially strengthen the manuscript.",
        "rating": "good",
        "confidence": "confident",
        "ethical_concerns": "No major ethical issues are apparent. All genomes are publicly available reference strains; no human subject data is involved."
    },
    {
        "strengths": "1. Novel framing: First comprehensive attempt to map from raw (KO-level) genomic content directly to community steady\u2013state composition, avoiding time\u2013series and explicit dynamical modeling.\n2. Well\u2013motivated use of GNNs\u2014permutation equivariance and ability to add/remove nodes matches the combinatorial nature of assemblages. Baselines (MLP w/ and w/out shuffling) convincingly illustrate this inductive bias.\n3. Empirical evaluation spans two independent wet-lab data sets (93 and 459 communities) and three OOD axes (larger communities, left-out species, shuffled order). The code and data are promised to be released, promoting reproducibility.\n4. Authors provide an open\u2010source gLV-based simulator that is configurable and used to probe hypotheses (edge density, keystone species, sample size, diversity), thereby acknowledging data scarcity and enabling controlled studies.\n5. Paper is generally well-written and the biological framing (e.g., discussion of keystones, higher-order effects) demonstrates welcome domain awareness.",
        "weaknesses": "1. OOD split design is partly confounded.\n   \u2022 When testing larger communities, species composition overlaps heavily with training; thus the task is mainly extrapolating interaction order, not unseen edges. Conversely, the \u2018unseen-species\u2019 split still re-uses every other species seen in training, giving the network substantial contextual information. A leave-k-species-out community split (no community containing any of the k species appears in training) would be stricter.\n   \u2022 5-fold CV is used for in-distribution tuning, but the OOD experiments reuse the same hyper-parameters without a validation set. Reported numbers might reflect luck in seed choice.\n2. Real data are tiny (\u226470 unique community structures in FRIEDMAN2017). Reported R\u00b2 ~0.8 could result from memorising a few characteristic abundance vectors. An ablation that withholds *community templates* (e.g., all AC+BT+DL combinations) is missing.\n3. Simulator is extremely stylised: gLV with pairwise interactions, no environmental variation, deterministic equilibrium, binary gene encoding with explicit bijection to parameters. Consequently, (i) it cannot test higher-order interaction failure modes the authors invoke to explain real-world errors, (ii) keystone behaviour is hard-wired via edge density only. Therefore the negative result in Fig 7B does not falsify the biological hypothesis\u2014it only shows the simulator does not capture it.\n4. Uncertainty quantification is limited to non-parametric bootstrap CIs on R\u00b2. Model-level predictive uncertainty (per-community/ per-species) is not reported, hindering assessment of when the model is trustworthy. No calibration curves, no Bayesian/ensemble variance or conformal interval.\n5. Interpretability is not pursued beyond qualitative anecdotes (\"non-Pseudomonas behave similarly\"). No gradient-based or GNN-explainer attribution to genomic features or message-passing edges is provided, so the claim that the method can \"provide insights\" is unsubstantiated.\n6. Ecological validity: Training on fully-connected graphs assumes all pairwise interactions exist. This biases the learned messages and conflates absence of evidence with evidence of absence. Sparse or learned adjacency variants should be tested.\n7. Over-claim: Authors state they \"show for the first time generalization to unseen bacteria\". The current evidence is mixed; accuracy for AC (keystone) is near zero. Claim should be softened.\n8. No discussion of potential mis-use (e.g., release of engineered consortia, dual-use design).",
        "comments": "Experimental design\n\u2022 In-distribution CV is sound, but for OOD claims a hierarchical splitting strategy is advisable: (i) choose test species set S_test; (ii) remove *all* communities containing any species in S_test from training/validation; (iii) tune on a validation set that also excludes S_test but includes other held-out species. This avoids subtle leakage.\n\u2022 Larger-community extrapolation could be assessed by holding out *all* communities whose size \u2265k rather than only 7\u20138 or 23\u201326; else the test set is small and skewed.\n\nSimulation-based validation\n\u2022 To probe higher-order effects, extend the simulator with third-order gLV terms or resource competition (e.g., MacArthur consumer-resource). Tune a synthetic \u2018complexity knob\u2019 and evaluate whether model accuracy degrades similarly to real data.\n\u2022 Keystone definition should go beyond degree: implement perturbation analysis (\u0394 in equilibrium when species is removed) and sample keystones accordingly; then repeat left-out-species experiment.\n\nUncertainty estimation\n\u2022 Report ensemble variance across seeds as a per-sample epistemic uncertainty signal.\n\u2022 Use deep ensembles or MC-Dropout at inference; compute calibration (ECE) and coverage of 95 % prediction intervals on abundance vectors.\n\u2022 Consider a Dirichlet likelihood with concentration parameters conditioned on the GNN output to model compositional data with inherent simplex constraints.\n\nInterpretability\n\u2022 Apply GNNExplainer / PGM-Explainer to identify which input genes or inter-node messages most influence each predicted abundance; correlate with known metabolic pathways (e.g., butyrate genes for AC).\n\u2022 Edge masking or attention weights could reveal candidate interaction matrices that may be compared with gLV fits.\n\nBroader impact & limitations\n\u2022 Predictive models could accelerate synthetic microbiome design for therapeutic or environmental applications, reducing experimental burden.\n\u2022 Risk: inaccurate predictions may lead to unintended ecological shifts when deployed. The paper should recommend laboratory validation pipelines and caution against direct clinical translation.\n\u2022 Dual-use: same tools could aid malicious engineering of harmful consortia; authors should acknowledge biosecurity screening and responsible disclosure practices.\n\nActionable suggestions\n1. Redesign OOD splits as outlined; report per-species ROC of presence/absence besides abundance to decouple extinction prediction.\n2. Provide calibration plots and per-community predictive intervals; flag test samples where ensemble variance is high.\n3. Incorporate at least one interpretability experiment (e.g., identify top-10 KEGG KOs driving prediction of BT abundance).\n4. Benchmark against a sparse-edge or attentional GNN that learns an adjacency matrix, and against a compositional regression baseline (e.g., Dirichlet-multinomial regression).\n5. Temper claims of \"for the first time\" and of successful unseen-species generalisation; highlight that performance is species-dependent and sometimes below mean baseline.\n6. Add a paragraph on ethical considerations and safe deployment practices.\n7. Release simulator and data at submission time (not post-acceptance) to facilitate reproducibility review.\n",
        "rating": "good",
        "confidence": "confident",
        "ethical_concerns": "Potential mis-use for bio-engineering harmful microbial consortia; ecological risk if models are applied directly without wet-lab validation. No human subject data involved. Authors should include a responsible use statement and recommend biosafety oversight."
    }
]