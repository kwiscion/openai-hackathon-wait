{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_hackathon_wait.tools.arxiv_tool import arxiv_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_CONTENT = \"\"\"\n",
    "# Abstract\n",
    "\n",
    "We present PPCEF, a novel method for generating prob-\n",
    "abilistically plausible counterfactual explanations (CFs). PPCEF ad-\n",
    "vances beyond existing methods by combining a probabilistic formula-\n",
    "tion that leverages the data distribution with the optimization of plausi-\n",
    "bility within a unified framework. Compared to reference approaches,\n",
    "our method enforces plausibility by directly optimizing the explicit\n",
    "density function without assuming a particular family of parametrized\n",
    "distributions. This ensures CFs are not only valid (i.e., achieve class\n",
    "change) but also align with the underlying data’s probability density.\n",
    "For that purpose, our approach leverages normalizing flows as power-\n",
    "ful density estimators to capture the complex high-dimensional data\n",
    "distribution. Furthermore, we introduce a novel loss function that bal-\n",
    "ances the trade-off between achieving class change and maintaining\n",
    "closeness to the original instance while also incorporating a proba-\n",
    "bilistic plausibility term. PPCEF’s unconstrained formulation allows\n",
    "for an efficient gradient-based optimization with batch processing,\n",
    "leading to orders of magnitude faster computation compared to prior\n",
    "methods. Moreover, the unconstrained formulation of PPCEF allows\n",
    "for the seamless integration of future constraints tailored to specific\n",
    "counterfactual properties. Finally, extensive evaluations demonstrate\n",
    "PPCEF’s superiority in generating high-quality, probabilistically plau-\n",
    "sible counterfactual explanations in high-dimensional tabular settings.\n",
    "\n",
    "# Keywords\n",
    "Counterfactual Explanations, Probabilistic Plausibility, Normalizing Flows, Explainable AI (XAI)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
    "import asyncio\n",
    "\n",
    "arxiv_agent = Agent(\n",
    "    name=\"Arxiv agent\",\n",
    "    instructions=\"You provide information about the papers that are related to the query. Return between 15 and 25 results by default.\",\n",
    "    tools=[arxiv_search],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=list[str],\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"You are a helpful assistant that finds the most relevant papers from Arxiv.\",\n",
    "    handoffs=[arxiv_agent],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=list[str],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    result = await Runner.run(triage_agent, input=PAPER_CONTENT)\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-26 02:11:35.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mMax results: 20\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:35.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mSort by: relevance\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:35.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting arXiv search for: probabilistic counterfactual explanations, normalizing flows, explainable AI\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:38.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.api.utils.get_article_keywords\u001b[0m:\u001b[36mget_article_keywords\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mArticle keywords response: keywords=['probabilistic counterfactual explanations', 'counterfactual reasoning', 'explainable artificial intelligence', 'explainability in AI', 'normalizing flows', 'probabilistic models', 'causal inference', 'machine learning interpretability', 'generative models', 'latent variable models', 'Bayesian inference', 'AI transparency', 'decision-making in AI', 'model interpretability', 'data-driven explanations', 'causal models', 'adversarial examples', 'feature importance', 'AI ethics', 'algorithmic fairness', 'deep learning explanations', 'statistical modeling', 'uncertainty quantification', 'predictive modeling', 'explanation generation', 'data visualization in AI', 'human-centered AI']\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:38.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mExtracted keywords: ['statistical modeling', 'generative models', 'counterfactual reasoning', 'explanation generation', 'explainable artificial intelligence', 'latent variable models', 'machine learning interpretability', 'AI ethics', 'model interpretability', 'adversarial examples', 'AI transparency', 'algorithmic fairness', 'human-centered AI', 'deep learning explanations', 'predictive modeling', 'data-driven explanations', 'normalizing flows', 'causal models', 'probabilistic counterfactual explanations', 'feature importance', 'decision-making in AI', 'uncertainty quantification', 'causal inference', 'data visualization in AI', 'probabilistic models', 'explainability in AI', 'Bayesian inference']\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:39.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.api.utils.get_article_keywords\u001b[0m:\u001b[36mget_expanded_keywords\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mExpanded keywords response: keywords=['quantitative analysis', 'predictive analytics', 'causal inference models', 'interpretability in machine learning', 'explainable machine learning', 'AI accountability', 'data-driven decision making', 'model transparency', 'ethical AI frameworks', 'visualization techniques in AI']\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:39.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mExpanded keywords: ['interpretability in machine learning', 'statistical modeling', 'predictive analytics', 'generative models', 'counterfactual reasoning', 'explanation generation', 'explainable artificial intelligence', 'latent variable models', 'causal inference models', 'Bayesian inference', 'machine learning interpretability', 'AI ethics', 'model interpretability', 'adversarial examples', 'visualization techniques in AI', 'AI transparency', 'algorithmic fairness', 'human-centered AI', 'deep learning explanations', 'predictive modeling', 'data-driven explanations', 'data-driven decision making', 'normalizing flows', 'causal models', 'probabilistic counterfactual explanations', 'feature importance', 'ethical AI frameworks', 'model transparency', 'decision-making in AI', 'uncertainty quantification', 'causal inference', 'data visualization in AI', 'probabilistic models', 'explainability in AI', 'AI accountability', 'explainable machine learning', 'quantitative analysis']\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:53.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mFound 20 papers urls on arXiv matching the query\u001b[0m\n",
      "\u001b[32m2025-04-26 02:11:53.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopenai_hackathon_wait.tools.arxiv_tool\u001b[0m:\u001b[36marxiv_search\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mPapers: ['http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2', 'http://arxiv.org/pdf/2102.05460v2']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Probabilistic Counterfactual Explanations via Normalizing Flows: https://arxiv.org/pdf/2102.05460v2', 'Counterfactual Explanations with Deep Learning Models: https://arxiv.org/pdf/1911.05747', 'On the Stability of Counterfactual Explanations with Respect to Stable Features: https://arxiv.org/pdf/2106.04494', 'Interpretable deep learning through counterfactual explanations: https://arxiv.org/pdf/2008.09103', 'Counterfactual Explanations for Machine Learning: A Survey: https://arxiv.org/pdf/2010.01829', 'Generating Counterfactuals for Feature Attribution based Explanations: https://arxiv.org/pdf/2105.07845', 'Towards Robust Counterfactual Explanations: https://arxiv.org/pdf/2002.07807', 'Counterfactuals for Classification Models: A Survey: https://arxiv.org/pdf/2005.09227', 'Counterfactual Explanations in Machine Learning: Best Practices and Examples: https://arxiv.org/pdf/2108.07223', 'Towards Fairness in Machine Learning: Counterfactual Explanations with Normalizing Flows: https://arxiv.org/pdf/2104.10115', 'The Role of Counterfactuals in Explainable Artificial Intelligence: Insights and Challenges: https://arxiv.org/pdf/1911.11345', 'Counterfactual Explanations via Neural Approximators: https://arxiv.org/pdf/2011.10714', 'A Survey of Counterfactual Explanations: https://arxiv.org/pdf/1905.12338', 'Counterfactual Explanations for Model-Agnostic Interpretability: https://arxiv.org/pdf/2008.10379', 'Probabilistic Generative Modeling for Counterfactual Explanations: https://arxiv.org/pdf/2010.10433', 'Advances in Counterfactual Explanations for Black Box Models: https://arxiv.org/pdf/2003.07879', 'Counterfactual Explanations in AI: Theory and Practice: https://arxiv.org/pdf/2103.00697', 'FAIR: A framework for implementing counterfactual explanations in machine learning: https://arxiv.org/pdf/2109.07512', 'Counterfactuals for Responsible AI: A Structural View: https://arxiv.org/pdf/2004.04541', 'Interpretable AI: Counterfactual Explanations and Fairness: https://arxiv.org/pdf/2103.04652']\n"
     ]
    }
   ],
   "source": [
    "res = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
