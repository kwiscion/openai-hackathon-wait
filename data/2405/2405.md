# Probabilistically Plausible Counterfactual Explanations with Normalizing Flows

Patryk Wielopolskia[,\\*](#page-0-0), Oleksii Furman<sup>a</sup> , Jerzy Stefanowski<sup>b</sup> and Maciej Zi˛ebaa,c

> <sup>a</sup>Wrocław University of Science and Technology <sup>b</sup>Poznan University of Technology ´ <sup>c</sup>Tooploox Sp. z o.o.

Abstract. We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs). PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework. Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions. This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data's probability density. For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution. Furthermore, we introduce a novel loss function that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term. PPCEF's unconstrained formulation allows for an efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods. Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties. Finally, extensive evaluations demonstrate PPCEF's superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings.

# 1 Introduction

Counterfactual explanations (briefly *counterfactuals*, and abbreviated as CF) are one particular type of such explanations of black box model predictions that provide information about how feature values of an example should be changed to obtain a more desired prediction of the model (i.e., to change its target decision) [\[30\]](#page-7-0). On the one hand, by interacting with the model using counterfactuals, the user can better understand how the system works by exploring "what would have happened if..." scenarios. On the other hand, a good counterfactual provides a practical recommendation to the user about what changes are needed in order to achieve the desired outcome.

There are many practical applications for counterfactual explanations, including loan or insurance decisions [\[31\]](#page-7-1), recruitment processes [\[21\]](#page-7-2), the discovery of chemical compounds [\[32\]](#page-7-3), medical diagnosis [\[17\]](#page-7-4), and many others, see, e.g., the recent survey [\[10\]](#page-7-5).

More formally, a counterfactual explanation is an alternative input instance, denoted as x ′ , which is minimally modified from the description of the original instance x0, such that the output of the classifier h

<span id="page-0-1"></span>![](_page_0_Figure_10.jpeg)

Figure 1: Probabilistically Plausibile Counterfactual Explanation Estimation Process on the Moons Dataset. We show an evolution of an instance from the initial instance (black dot) to the final counterfactual (red dot) against the linear classifier's decision boundary (blue line) and density threshold contours, highlighting the method's trajectory towards achieving target classification and probabilistic plausibility condition.

changes from the original decision y = h(x0) to a specific desired outcome y ′ = h(x ′ ).

Up to now, several algorithms for generating counterfactual explanations have been introduced. They are based on different principles, and for comprehensive surveys, see, e.g., [\[10,](#page-7-5) [30\]](#page-7-0). Depending on the specific method, some properties of counterfactuals are expected to be met, such as *validity* of the decision change, *proximity* to the input instance, *sparsity* of recommended changes, their *actionability*, i.e., the counterfactual should not modify immutable features or violate monotonic constraints, and *plausibility* of locating the counterfactual within a high-density region of the data, ensuring that the proposed counterfactuals are realistic and feasible within the context of the observed data distribution.

Many of these methods are inspired by the formulation of Wachter et al. [\[31\]](#page-7-1), which proposed framing counterfactual explanations as an unconstrained optimization problem. For a prediction function h and an input x<sup>0</sup> ∈ R d , a counterfactual x ′ ∈ R d is computed by solving:

$$\arg\min\_{\mathbf{x}' \in \mathbb{R}^d} \ell(h(\mathbf{x}'), y') + C \cdot d(\mathbf{x}\_0, \mathbf{x}'). \tag{l}$$

<span id="page-0-0"></span><sup>∗</sup> Corresponding Author. Email: patryk.wielopolski@pwr.edu.pl.

In this formulation, ℓ(·, ·) represents a classification loss function, d(·, ·) is a penalty for deviation from the original input x0, and the term C ≥ 0 serves as the regularization strength modifier.

An alternative approach [\[2\]](#page-7-6) frames counterfactual explanations as a constrained optimization problem. This perspective focuses on directly finding the minimal perturbation required to achieve the target prediction under the constraint that the model's prediction for the counterfactual instance meets the specified criterion. Mathematically, this is represented as:

<span id="page-1-0"></span>
$$\arg\min\_{\mathbf{x}' \in \mathbb{R}^d} d(\mathbf{x}\_0, \mathbf{x}') \quad \text{s.t.} \quad h(\mathbf{x}') = y'. \tag{2}$$

In our study, we want to pay special attention to the *plausibility* of counterfactuals. Referring to arguments of [\[10\]](#page-7-5), a counterfactual is plausible if the feature values describing the example are coherent (sufficiently similar) with those present in the original data X. This means it should be located in sufficiently dense regions of original instances in X from the target class. Plausibility helps in increasing users' trust in the explanation: it would be hard to trust a counterfactual if it is a combination of features that are unrealistic with respect to existing examples.

In previous works, plausibility has often been verified by simple k-neighbourhood analysis of the counterfactual with respect to the original data [\[26,](#page-7-7) [14,](#page-7-8) [27\]](#page-7-9). Few other approaches [\[3\]](#page-7-10) model the conditional density in the target class and try to find the counterfactual example with the density value above the given threshold. Although the problem is quite well mathematically defined, the current methods apply simple approaches like kernel density estimators or a mixture of Gaussians to model conditional distributions that are difficult to apply for high-dimensional data. Moreover, the problem of estimating valid and plausible counterfactuals is defined as a complex constrained optimization problem with strict convexity assumptions [\[3\]](#page-7-10). Finally, the currently proposed methods, while providing valid counterfactuals, struggle to consistently produce observations that fulfill the plausibility criteria.

In this paper, we introduce PPCEF: Probabilistically Plausibile Counterfactual Explanations using Normalizing Flows - a novel approach to estimate counterfactual explanations for differentiable classifiers tailored for tabular problems. It includes a novel, unconstrained formulation of the problem that enables direct estimation of the plausibility property - to the best of our knowledge, a characteristic previously not achieved in the literature. For that purpose, we design loss functions to satisfy both validity and plausibility constraints and minimize the distance to the original example in a balanced way (see an example in Fig. [1\)](#page-0-1). Our approach incorporates plausibility in the probabilistic sense by targeting observations with a probability density exceeding a predefined threshold [\[3\]](#page-7-10). Unlike existing methods limited to specific estimators of families of density functions, ours employs any differentiable conditional density model. Moreover, we postulate to utilize *conditional normalizing flows* for density estimation [\[24\]](#page-7-11), ensuring independence from specific parameterized distribution families while enabling direct calculation of density values for complex, high-dimensional data. Finally, PPCEF leverages efficient batch processing utilizing gradient-based optimization techniques, leading to significant computational gains compared to previous methods.

To summarize, our contributions are as follows:

- The formulation of counterfactual explanations within an unconstrained optimization framework employing direct optimization of plausibility and novel loss functions.
- The utilization of normalizing flows as density estimators to capture the complex high-dimensional data distribution effectively.

• The experimental evaluations demonstrating PPCEF's ability to efficiently generate high-quality, probabilistically plausible counterfactuals in high-dimensional tabular datasets for both binary and multiclass classification problems, outperforming existing reference methods.

# 2 Related Works

# *2.1 Plausible Counterfactual Explanations*

The approaches for obtaining plausible counterfactual explanations are primarily categorized into *endogenous* and *exogenous* ones [\[10\]](#page-7-5). Endogenous counterfactuals are crafted using feature values from existing data instances, ensuring their naturally occurring status and grounding them in real-world contexts, thereby enhancing their plausibility. In contrast, exogenous counterfactuals are generated through methods such as interpolations or random data generation, which do not strictly rely on existing dataset features. While this offers greater flexibility, it does not inherently assure the plausibility of these counterfactuals, as they might represent feature combinations not found in actual data.

# *2.1.1 Endogenous Counterfactual Explanations*

Endogenous approaches to counterfactual explanations revolve around leveraging existing instances within the dataset to generate plausible counterfactuals. These methods, which include instance-based or casebased approaches, primarily utilize nearest neighbors' techniques to identify instances that closely resemble the input but yield different outcomes.

Examples of endogenous methods include the Nearest-Neighbor Counterfactual Explainer (NNCE) [\[26\]](#page-7-7), selecting similar yet outcomedivergent instances from the dataset as counterfactuals. The Case-Based Counterfactual Explainer (CBCE) [\[14\]](#page-7-8) forms 'explanation cases' by pairing similar instances with contrasting outcomes, creating counterfactuals by merging features from these pairs. Extending this concept, the approach by Smyth and Keane [\[27\]](#page-7-9) adapts to knearest neighbors, utilizing multiple nearest candidates for generating counterfactuals. Feasible and Actionable Counterfactual Explanations (FACE) [\[23\]](#page-7-12) constructs a graph over data points, applying user-defined parameters to find actionable paths to desired outcomes. Lastly, PRO-PLACE [\[12\]](#page-7-13) employs bi-level optimization and Mixed-Integer Linear Programming, generating robust counterfactuals from ∆-robust nearest neighbors that closely align with data distribution and model robustness.

# *2.1.2 Exogenous Counterfactual Explanations*

In the landscape of exogenous counterfactual explanations, methods generally involve introducing external modifications to original instances, diverging from reliance on existing instances and their features. These approaches utilize a range of computational techniques, such as autoencoders, linear programming, gradient-based methods, and generative models, to ensure that the resulting counterfactuals are plausible.

Firstly, the Contrastive Explanation Method (CEM) [\[5\]](#page-7-14) innovates by adding perturbations to an instance and utilizing an autoencoder to verify the closeness of the modified instance to known data, ensuring plausibility. Meanwhile, the Diverse Coherent Explanations (DCE) [\[25\]](#page-7-15) method leverages linear programming to create varied counterfactuals, with additional linear constraints to maintain both

diversity and plausibility. Further, the Distribution-Aware Counterfactual Explanation (DACE) [\[13\]](#page-7-16) method incorporates the Mahalanobis distance and Local Outlier Factor (LOF) in its loss function, focusing on minimizing this distance while keeping a low LOF score to signify higher plausibility. The Diverse Counterfactual Explanations (DICE) [\[18\]](#page-7-17) approach involves solving an optimization problem to generate multiple counterfactuals, with a specific emphasis on the diversity and actionability of these counterfactuals to determine their plausibility. Additionally, Counterfactual Explanations Guided by Prototypes (CEGP) [\[15\]](#page-7-18) adopts a similar loss function to CEM but introduces a prototype-based loss term. This guides perturbations towards a counterfactual that aligns with the data distribution of a specific class, using the encoder of an autoencoder based on the average encoding of the nearest instances in the latent space with the same class label.

Within the field of exogenous counterfactual explanations, a subcategory particularly relevant to our work utilizes deep generative models. Variational Autoencoders (VAEs) are exploited in methods like Example-Based Counterfactual (EBCF) [\[16\]](#page-7-19) and the approach by Vercheval and Pizurica [\[29\]](#page-7-20). EBCF incorporates known causal relationships into the VAE, promoting realistic counterfactuals. The method by Vercheval and Pizurica [\[29\]](#page-7-20) enables visual counterfactual generation through VAE-based latent space exploration. Generative Adversarial Networks (GANs) play a crucial role in the PCATTGAN approach [\[1\]](#page-7-21). It utilizes adversarial examples within a multi-objective optimization framework to create plausible counterfactuals, considering validity, minimality, and a notion of plausibility defined as human-understandable, non-automated changes. Diffusion models underpin methods proposed in [\[11,](#page-7-22) [4\]](#page-7-23). While these approaches specialize in visual counterfactual generation, their focus lies primarily on counterfactual sampling, not controlling plausibility via densitybased optimization. Lastly, Normalizing Flow-based methods [\[8,](#page-7-24) [9\]](#page-7-25) center on pinpointing counterfactuals within their latent spaces. These methods leverage the invertible nature of normalizing flows to explore counterfactual regions in the latent representation of the data.

All of the reference methods, except Artelt and Hammer [\[3\]](#page-7-10), do not provide an explicit probabilistic formulations of plausibility. Compared to Artelt and Hammer [\[3\]](#page-7-10), we propose an alternative problem formulation in unconstrained form with no prior constraints on the density model.

#### 3 Background

In this work, we consider the problem formulation of probabilistically plausible counterfactual explanations introduced by Artelt and Hammer [\[3\]](#page-7-10). This approach extends the problem formulation given by eq. [\(2\)](#page-1-0) by adding a target-specific density constraint to enforce the plausibility of counterfactuals using a probabilistic framework. The constrained optimization problem is formulated as follows:

$$\arg\min\_{\mathbf{x'} \in \mathbb{R}^d} d(\mathbf{x}\_0, \mathbf{x'}) \tag{3a}$$

$$\text{s.t.}\quad h(\mathbf{x'}) = y'\tag{3b}$$

$$
\delta \le p(\mathbf{x}' | y'),
\tag{3c}
$$

where p(x ′ |y ′ ) denotes conditional probability of the counterfactual explanation x ′ under desired target class value y ′ and δ represents the density threshold.

This approach's crucial aspect is finding the proper model to represent the conditional density function p(x|y). Typically, kernel density estimators (KDEs) are used to model conditional densities, but the use of non-linear kernels results in the highly non-convex optimization problem formulation. Gaussian Mixture Model (GMM) can be

applied alternatively, but convexity constraints are still not satisfied. To facilitate the desired optimization process, the authors of Artelt and Hammer [\[3\]](#page-7-10) propose to approximate the density value p(x ′ |y ′ ) using a component-wise maximum of GMM components:

$$\hat{p}\_G(\mathbf{x}'|y') = \max\_j \left( \pi\_{\boldsymbol{\upbeta},y'} \mathcal{N}(\mathbf{x}'|\boldsymbol{\upmu}\_{\boldsymbol{\upbeta},y'}, \boldsymbol{\upSigma}\_{\boldsymbol{\upbeta},y'}) \right), \tag{4}$$

where µj,y′ , Σj,y′ and πj,y′ are means, covariances and prior values for component j considering class y.

This approximation is transformed into a convex quadratic constraint for each GMM component j, resulting in the following formula:

$$(\mathbf{x}' - \boldsymbol{\mu}\_{j,y'})^T \boldsymbol{\Sigma}\_{j,y'} (\mathbf{x}' - \boldsymbol{\mu}\_{j,y'}) + c\_j \le \delta',\tag{5}$$

where c<sup>j</sup> is constant from the Gaussian normalization factor and δ ′ = −2 log δ.

For each component j, the optimization problem is solved, resulting in a set of convex programs - one for each component. This step is crucial because knowing beforehand which component will produce a feasible and plausible counterfactual is impossible. Finally, the counterfactual x ′ that yields the smallest value for the objective function is selected.

However, this approach has few limitations. First, the number of components should be predefined for each class. Second, the family of parametrized distributions limits the ability to adjust to a data distribution. Third, the approach is difficult to be applied to highdimensional data due to the Gaussian components.

In order to cope with the listed limitations, we postulate to model conditional density function p(x|y) using the normalizing flows [\[24\]](#page-7-11). This group of models can adjust to very complex, high-dimensional data distributions, which allows for calculating the density value from the change-of-variable formula. Moreover, we propose an alternative unconstrained problem formulation that allows solving using a gradient-based approach for any differentiable representation of conditional distribution p(x|y).

# 4 Method

This section introduces a novel approach to the problem of plausible counterfactual explanation formulated by eq. [\(3\)](#page-2-0). First, we reformulate the problem of calculating counterfactuals as unconstrained optimization suitable for direct, gradient-based optimization. Next, we show how to train the flow model to estimate the class-conditional distributions. Finally, we show how the counterfactuals can be efficiently estimated using a gradient-based approach.

# *4.1 Unconstrained Probabilistically Plausible Counterfactual Explanations*

<span id="page-2-0"></span>We consider a binary classification problem, y ∈ {0, 1}. However, our considerations can be easily extended to the multiclass case. Further, we consider a discriminative differentiable model (e.g., Logistic Regression or MLP) pd(y|x) and reformulate the validity constraint h(x ′ ) = y ′ as pd(y ′ |x ′ ) ≥ 0.5 + ϵ, where ϵ → 0, practically represented as small enough value close to 0.

We postulate the following unconstrained optimization problem:

<span id="page-2-1"></span>
$$\arg\min\_{\mathbf{x}' \in \mathbb{R}^d} d(\mathbf{x}\_0, \mathbf{x}') + \lambda \cdot \left(\ell\_v(\mathbf{x}', y') + \ell\_p(\mathbf{x}', y')\right), \qquad (6)$$

where λ = ∞, practically, is large enough.

The loss ℓv(x ′ , y′ ) component controls the validity constraint and is defined as follows:

$$\ell\_v(\mathbf{x}', y') = \max\left(0.5 + \epsilon - p\_d(y'|\mathbf{x}'), 0\right). \tag{7}$$

The Binary Cross Entropy (BCE) criterion can be used alternatively. However, using such criteria enforces 100% confidence of the discriminative model, while our approach aims at achieving the current classification accuracy with the margin controlled with the ϵ parameter. While using our criterion, the model can focus more on producing closer and more plausible counterfactuals, which we show in ablation studies.

Additionally, we extend the validity loss component to the multiclass scenario in the following way:

$$\ell\_v(\mathbf{x}', y') = \max \left( \max\_{y \neq y'} p\_d(y|\mathbf{x}') + \epsilon - p\_d(y'|\mathbf{x}'), 0 \right), \quad (8)$$

where we replace the 0.5 threshold value with the highest probability value returned by the discriminative model, excluding the value for target class y ′ . This guarantees that pd(y|x ′ ) will be higher than the most probable class among the remaining classes by the ϵ margin.

The loss component ℓp(x ′ , y′ ) controls probabilistic plausibility constraint (δ ≤ p(x ′ |y ′ )) and is defined as:

$$\ell\_p(\mathbf{x}', y') = \max\left(\delta - p(\mathbf{x}'|y'), 0\right),\tag{9}$$

where δ is the density threshold calculated in the same way as in [\[3\]](#page-7-10), i.e., by utilizing the median of the training dataset. The conditional distribution p(x|y) can be represented by any differentiable model (e.g., Mixture of Gaussians, KDE). In this work, we postulate to model the distribution using conditional normalizing flow due to the flexibility and ability to adjust to multidimensional complex distributions. Thanks to the unconstrained problem formulation given by eq. [\(6\)](#page-2-1) and differentiation assumption for the models, the counterfactuals can be easily calculated using a gradient-based approach.

#### <span id="page-3-2"></span>*4.2 Probabistically Plausible Counterfactual Explanations via Normalizing Flow-based Density Estimation*

KDE or GMMs can be used to model the conditional distributions. However, those models have limited modeling capabilities due to the parametrized (usually Gaussian) form of p(x|y) or the inability to model high-dimensional data (KDE). Therefore, in this work, we postulate the use of a conditional normalizing flow model [\[24\]](#page-7-11) to estimate the density for the joint distribution of the attributes for each class.

Normalizing Flows have surged in popularity within generative models due to their adaptability and the simplicity of training via direct negative log-likelihood (NLL) optimization. Their adaptability stems from the change-of-variable technique, which transforms a latent variable z with a known prior distribution p(z) into an observed space variable x with an unknown distribution. This transformation occurs through a sequence of invertible (parametric) functions: x = f<sup>K</sup> ◦· · ·◦f1(z, y). Assuming a known prior p(z) for z, the conditional log-likelihood for x is expressed as:

<span id="page-3-0"></span>
$$\log \hat{p}\_F(\mathbf{x}|y) = \log p(\mathbf{z}) - \sum\_{k=1}^{K} \log \left| \det \frac{\partial \mathbf{f}\_k}{\partial \mathbf{z}\_{k-1}} \right|, \qquad (10)$$

where z = f −1 <sup>1</sup> ◦ · · · ◦ f −1 <sup>K</sup> (x, y) is a result of the invertible mapping. The biggest challenge in normalizing flows is the choice of the invertible functions fK, . . . ,f1. Several solutions have been proposed in

the literature to address this issue with notable approaches, including NICE [\[6\]](#page-7-26), RealNVP [\[7\]](#page-7-27), and MAF [\[19\]](#page-7-28).

For a given training set D = {(xn, yn)} N <sup>n</sup>=1 we simply train the conditional normalizing flow by minimizing negative log-likelihood:

$$Q = -\sum\_{n=1}^{N} \log \hat{p}\_F(\mathbf{x}\_n | y\_n),\tag{11}$$

where log ˆp<sup>F</sup> (xn|yn) is defined by eq. [\(10\)](#page-3-0). The model is trained using a gradient-based approach applied to the flow parameters stored in f<sup>k</sup> functions.

#### *4.3 Estimating Counterfactuals*

For a trained conditional normalizing flow, the counterfactual explanation can be easily calculated simply by optimizing the criterion given by eq. [\(6\)](#page-2-1). The parameters of the flow model are frozen, and x ′ is optimized using the gradient-based procedure, starting from the point x0. To enhance the efficiency of our method, we have incorporated batch processing capabilities, allowing for the simultaneous calculation of multiple counterfactual explanations. This is achieved by aggregating instances and employing an average aggregation for loss calculation. Such a feature is notably absent in the other approaches compared to this study, providing our method with a distinct computational advantage.

#### 5 Experiments

In this section, we aim to demonstrate and validate our counterfactual explanation method through a series of experiments. Initially, we illustrate our method's intuition with the Moons dataset and Logistic Regression model. Next, we compare our approach against the only reference method in a probabilistically plausible CFs area - Artelt and Hammer [\[3\]](#page-7-10), as well as other established CF methods. This comparison focuses on the impact of plausibility on proximity metrics and time efficiency. Lastly, we conduct broader comparisons using other classifier models: Logistic Regression (LR), Multilayer Perceptron (MLP), and Neural Oblivious Decision Ensembles (NODE) [\[22\]](#page-7-29). The code for these experiments is publicly released on GitHub[1](#page-3-1) .

Datasets To evaluate PPCEF's effectiveness, we conducted experiments on seven numerical-only tabular datasets. Four datasets (Law, Heloc, Moons, and Audit) represent binary classification problems, whereas the first two datasets (Law and Heloc) are commonly used benchmarks for counterfactual explanation tasks. The remaining three datasets (Blobs, Digits, and Wine) address multiclass classification problems. Detailed descriptions of these datasets are available in the Appendix [B](#page-12-0) [\[33\]](#page-7-30). Overall, they represent broad diversity in sample sizes (up to approximately 10.000), number of variables (up to 64), and number of classes (up to 10). For preprocessing purposes, we implemented two key steps to prepare the datasets. First, we addressed class imbalance by downsampling the majority class to match the size of the minority class. Second, we normalized all features across the datasets to a [0, 1] range, enabling consistent scale and comparability among features. Thirdly, to ensure robust method evaluation, we employed stratified 5-fold cross-validation on each dataset. Finally, for clarity, the main manuscript reports average values, while the appendix [\[33\]](#page-7-30) includes standard deviation for detailed analysis.

<span id="page-3-1"></span><sup>1</sup> <https://github.com/ofurman/counterfactuals>

<span id="page-4-0"></span>Table 1: Comparative Results of Probabilistically Plausible Counterfactual Explanation Methods. We contrast the performance of PPCEF method with Artelt & Hammer [\[3\]](#page-7-10) and other methods across Logistic Regression (LR) classifier. The results demonstrate our method's consistently valid and probabilistically plausible results and its ability to produce counterfactuals even in complex scenarios like high-dimensional data.

| DATASET | METHOD | COVERAGE ↑ | VALIDITY ↑ | PROB. PLAUS. ↑ | LOF      | ISOFOREST | LOG DENS. ↑ | L1 ↓  | L2 ↓ | TIME ↓    |
|---------|--------|------------|------------|----------------|----------|-----------|-------------|-------|------|-----------|
| MOONS   | CBCE   | 1.00       | 1.00       | 0.10           | 1.06     | 0.03      | -5.81       | 0.62  | 0.48 | 0.07 S    |
|         | CEGP   | 1.00       | 1.00       | 0.09           | 1.36     | 0.00      | -6.66       | 0.36  | 0.28 | 904.11 S  |
|         | CEM    | 1.00       | 1.00       | 0.14           | 2.03     | -0.07     | -10.09      | 0.55  | 0.50 | 211.56 S  |
|         | WACH   | 0.98       | 1.00       | 0.11           | 1.55     | -0.01     | -6.34       | 0.49  | 0.36 | 198.29 S  |
|         | ARTELT | 1.00       | 1.00       | 0.08           | 1.53     | -0.03     | -8.74       | 0.32  | 0.32 | 4.15 S    |
|         | PPCEF  | 1.00       | 1.00       | 1.00           | 1.01     | 0.04      | 1.69        | 0.45  | 0.36 | 1.85 S    |
| LAW     | CBCE   | 1.00       | 1.00       | 0.49           | 1.05     | 0.04      | 1.28        | 0.61  | 0.40 | 0.23 S    |
|         | CEGP   | 1.00       | 1.00       | 0.49           | 1.07     | 0.04      | 1.08        | 0.23  | 0.18 | 1973.76 S |
|         | CEM    | 1.00       | 1.00       | 0.26           | 1.26     | -0.02     | -0.56       | 0.33  | 0.31 | 368.10 S  |
|         | WACH   | 1.00       | 1.00       | 0.39           | 1.30     | -0.01     | -0.29       | 0.45  | 0.35 | 359.00 S  |
|         | ARTELT | 1.00       | 1.00       | 0.40           | 1.12     | 0.02      | 0.54        | 0.20  | 0.20 | 4.02 S    |
|         | PPCEF  | 1.00       | 1.00       | 1.00           | 1.03     | 0.07      | 2.05        | 0.37  | 0.23 | 2.42 S    |
| AUDIT   | CBCE   | 1.00       | 1.00       | 0.79           | 11.70    | 0.14      | 54.97       | 2.55  | 1.24 | 0.04 S    |
|         | CEGP   | 0.97       | 1.00       | 0.02           | 6.08·107 | 0.02      | 8.09        | 1.56  | 0.57 | 561.04 S  |
|         | CEM    | 0.52       | 1.00       | 0.00           | 8.28·106 | -0.04     | 20.84       | 1.20  | 0.37 | 105.92 S  |
|         | WACH   | 0.99       | 1.00       | 0.02           | 1.42·108 | 0.06      | -40.34      | 1.78  | 0.80 | 101.27 S  |
|         | ARTELT | 0.60       | 0.97       | 0.00           | 4.09·108 | 0.10      | -3585.76    | 0.90  | 0.88 | 43.84 S   |
|         | PPCEF  | 1.00       | 0.99       | 0.99           | 4.25·107 | 0.08      | 51.64       | 2.04  | 0.79 | 7.01 S    |
| HELOC   | CBCE   | 1.00       | 1.00       | 0.54           | 1.10     | 0.07      | 28.01       | 2.84  | 0.82 | 5.71 S    |
|         | CEGP   | 1.00       | 1.00       | 0.29           | 3.50·107 | 0.04      | 24.75       | 0.26  | 0.10 | 9654.60 S |
|         | CEM    | 1.00       | 1.00       | 0.07           | 2.50·108 | 0.02      | 12.37       | 0.35  | 0.20 | 1639.16 S |
|         | WACH   | 1.00       | 1.00       | 0.00           | 2.65·108 | 0.03      | -15.09      | 0.74  | 0.37 | 1600.28 S |
|         | ARTELT | 0.00       | -          | -              | -        | -         | -           | -     | -    | - S       |
|         | PPCEF  | 1.00       | 1.00       | 1.00           | 6.47·107 | 0.07      | 32.42       | 0.90  | 0.23 | 12.44 S   |
| BLOBS   | CBCE   | 1.00       | 1.00       | 0.27           | 1.02     | 0.03      | -35.52      | 0.95  | 0.72 | 0.13 S    |
|         | CEGP   | 1.00       | 1.00       | 0.00           | 2.43     | -0.07     | -9.08       | 0.30  | 0.25 | 1295.36 S |
|         | CEM    | 0.96       | 1.00       | 0.00           | 3.51     | -0.12     | -14.95      | 0.46  | 0.45 | 512.56 S  |
|         | WACH   | 1.00       | 1.00       | 0.04           | 2.24     | -0.06     | -9.52       | 0.51  | 0.38 | 441.59 S  |
|         | ARTELT | 1.00       | 1.00       | 0.00           | 2.11     | -0.07     | -3.51       | 0.39  | 0.33 | 6.62 S    |
|         | PPCEF  | 1.00       | 1.00       | 1.00           | 1.01     | 0.04      | 3.00        | 0.69  | 0.50 | 3.22 S    |
| DIGITS  | CBCE   | 1.00       | 1.00       | 0.18           | 1.02     | 0.04      | 23.72       | 16.28 | 3.09 | 0.51 S    |
|         | CEGP   | 1.00       | 1.00       | 0.11           | 1.09     | 0.01      | -0.39       | 2.53  | 0.63 | 1945.67 S |
|         | CEM    | 1.00       | 0.98       | 0.01           | 1.23     | -0.03     | -86.77      | 5.28  | 1.38 | 852.05 S  |
|         | WACH   | 1.00       | 1.00       | 0.08           | 1.20     | 0.00      | -34.97      | 2.47  | 1.20 | 651.00 S  |
|         | ARTELT | 0.80       | 0.93       | 0.04           | 1.69     | 0.01      | -54.72      | 3.30  | 2.43 | 238.28 S  |
|         | PPCEF  | 1.00       | 1.00       | 1.00           | 1.12     | 0.03      | 44.42       | 8.27  | 1.33 | 8.68 S    |
| WINE    | CBCE   | 1.00       | 1.00       | 0.37           | 1.06     | 0.05      | 2.13        | 3.38  | 1.12 | 0.01 S    |
|         | CEGP   | 1.00       | 1.00       | 0.01           | 1.08     | 0.05      | -0.15       | 0.82  | 0.32 | 191.09 S  |
|         | CEM    | 1.00       | 1.00       | 0.00           | 1.35     | -0.02     | -12.94      | 1.20  | 0.63 | 81.33 S   |
|         | WACH   | 1.00       | 1.00       | 0.01           | 1.27     | 0.00      | -9.41       | 1.57  | 0.78 | 50.74 S   |
|         | ARTELT | 1.00       | 0.97       | 0.01           | 1.33     | 0.02      | -11.73      | 0.68  | 0.65 | 0.96 S    |
|         | PPCEF  | 1.00       | 1.00       | 1.00           | 1.01     | 0.09      | 9.72        | 1.65  | 0.53 | 2.03 S    |

Classification Models For the experiments, we include Logistic Regression (LR), 3-layer Multilayer Perceptron (MLP), and Neural Oblivious Decision Ensemble (NODE) catering to both linear and non-linear scenarios. LR aligns with linear assumptions prevalent in some baseline methods, MLP allows for the assessment of behaviors in non-linear model contexts, and NODE stands as an example of a complex ensemble of neural decision trees. This triple-model approach facilitates a thorough evaluation across varied model complexities. Crucially, all models are differentiable, which is essential in the context of our method.

Experiments Details For every combination of the classification model and dataset, we trained both the classification model and a Normalizing Flow as the density estimator, following the approach detailed in Section [4.2.](#page-3-2) We opted for the Masked Autoregressive Flow (MAF) architecture [\[19\]](#page-7-28) as our choice for the Normalizing Flow. This decision was based on experimental findings indicating MAF's superior performance in accurately fitting data distributions. For a deeper analysis of these results, including in-depth model performance metrics like accuracy, please refer to the Appendix [\[33\]](#page-7-30). See Section [C](#page-12-1) for a detailed exploration and Tab. [9](#page-12-2) for specific performance figures. The final step involved generating counterfactual explanations for the entire set of test samples.

Reference Methods Our analysis includes several significant baselines, each selected for its relevance to the field. We first consider the method developed by Artelt and Hammer [\[3\]](#page-7-10), notable for its focus on probabilistically plausible counterfactuals. Additionally, we evaluate the approach by Wachter et al. [\[31\]](#page-7-1), widely recognized as a foundational baseline in counterfactual explanations research. To provide both endogenous and exogenous counterfactual explanations, we compare three methods: Case-Based Counterfactual Explainer (CBCE) [\[14\]](#page-7-8), Contrastive Explanation Method (CEM) [\[5\]](#page-7-14), and Counterfactual Explanations Guided by Prototypes (CEGP) [\[15\]](#page-7-18).

Metrics Following related works, we chose a comprehensive set of metrics to assess the performance of counterfactual explanation methods. We include two success metrics: *coverage*, evaluating the method's ability to generate explanations across all instances, and *validity*, assessing the efficacy of counterfactuals in altering the model's decision. In terms of proximity, we measure the *L1* and *L2* distances to quantify the closeness between original instances and their counterfactuals. We evaluate plausibility using a combination of metrics. First of all, we measure the *Local Outlier Factor (LOF)* score, which, when significantly greater than 1, indicates an outlier, with values closer to 1 suggesting normalcy, highlighting anomalies through local density deviations. Secondly, we utilize *Isolation Forest*, which

assigns scores between -0.5 and 0.5, with values approaching -0.5 identifying anomalies due to the ease of isolation and scores above 0 indicating normal observations. We further access counterfactuals using *probabilistic plausibility* metric, the proportion of CFs meeting the criterion defined in Eq. [3c.](#page-2-0) Moreover, we calculate *log density*, which gauges the logarithmic probability density of counterfactuals under the target class, with higher values indicating greater plausibility. Finally, we calculate *time* metric representing time in seconds needed for the method to process the whole test dataset.

#### *5.1 Method Intuition via Toy Example*

In our illustrative example, we present the counterfactual generation process using the Moons dataset under a Logistic Regression model, as depicted in Figure [1.](#page-0-1) The initial observation is represented by a black dot, with intermediary observations during the optimization process (after every 150 iteration steps) shown as orange dots and the final counterfactual outcome marked by a red dot. The probability distributions are indicated by contour lines, with the filled red contour denoting the region exceeding the desired density threshold. The blue line illustrates the decision boundary of the classifier. This visualization effectively demonstrates how our method navigates toward the target classification and probabilistic plausibility regions, adjusting its trajectory to surpass the classifier's decision boundary by a predefined margin ϵ upon achieving the required density level.

#### *5.2 Probabilistically Plausibile Counterfactual Explanations Methods Comparison*

In this section, we conduct a focused comparison of our approach, PPCEF, against the method by Artelt and Hammer [\[3\]](#page-7-10), which is the primary reference in the realm of probabilistically plausible counterfactual explanations. For that purpose, we utilize the datasets, metrics, and classifiers described in the previous section. The evaluation is centered on assessing and validating the accuracy of both methods in generating counterfactuals, their plausibility, and their proximity to original instances.

The results are presented in Tab. [1.](#page-4-0) Firstly, we can observe that our method always returns the results that are probabilistically plausible. That is not the case for Artelt's method, which struggles in high-dimensional datasets like Heloc (23 dimensions) or Digits (64 dimensions), doesn't support non-linear classifiers like MLPs, and wasn't able to consistently fulfill the probabilistic plausibility criterion. Secondly, in terms of distances, Artelt's method returns better results, which is expected due to the trade-off between distance and plausibility, i.e., the more plausible observations, the farther away they usually are. However, the results are not clearly worse, especially in terms of L2 distance, meaning PPCEF can balance both desired properties of counterfactuals. Thirdly, the log density values of the observations produced by PPCEF method are significantly better. Fourthly, our analysis using Local Outlier Factor (LOF) and Isolation Forest (Iso-Forest) metrics indicates that our methods generate inliers (except for Audit and Heloc, where almost all methods struggle to obtain reasonable values of LOF), whereas Artelt's method underperforms and can sometimes result in outliers. Fifthly, our method turned out to be significantly faster, with the speed up around x2-10 on relatively small datasets. Finally, our method was almost always able to produce valid counterfactual explanations for MLP and NODE, contrary to Artelt (see results in Tab. [2](#page-6-0) and detailed results in Tab. [6](#page-10-0) and [7](#page-11-0) in Appendix [\[33\]](#page-7-30)). It's worth mentioning that PPCEF almost always returned probabilistically plausible observations, which, in case of

non-valid observations, might still be valuable insight for the final user, contrary to the lack of a response at all.

#### *5.3 Counterfactual Explanations Methods Comparison*

In this comparative analysis, we evaluate our method against wellestablished reference methods, with a particular focus on the impact of integrating probabilistically plausible conditions into the optimization process. Our primary objective is to assess our method's performance in terms of validity, plausibility, proximity metrics, and processing efficiency. We also explore whether methods not specifically designed for plausibility can still produce plausible counterfactuals across various classifiers such as Logistic Regression (LR), Multilayer Perceptron (MLP), and Neural Oblivious Decision Ensembles (NODE).

Results presented in Tab. [1](#page-4-0) and Tab. [2](#page-6-0) indicate that our model excels in validity, plausibility (considering both probabilistic formulation and outlier metrics), and processing times while maintaining reasonable distances compared to competing approaches across all datasets and classification methods. Specifically, Tab. [2](#page-6-0) presents the evaluation results for two selected high-dimensional datasets (one for binary classification problem and one for multiclass problem) using two advanced classifiers, demonstrating that our method consistently produces valid results not only with a shallow model, such as LR but also with deeper models, including MLP and NODE. In contrast, the majority of existing methods encounter difficulties in producing valid counterfactual explanations for the Multilayer Perceptron. We conducted a comprehensive evaluation utilizing all methods and datasets mentioned earlier, applying three different classifiers. Detailed outcomes are presented in Appendix [A](#page-8-0) [\[33\]](#page-7-30). Particularly, results for Logistic Regression are shown in Tab. [5,](#page-9-0) while findings for the MLP and NODE classifiers are detailed in Tab. [6](#page-10-0) and [7,](#page-11-0) respectively.

Furthermore, our hypothesis that reference methods could inadvertently yield plausible outcomes without targeted optimization was not confirmed. In terms of proximity, CEGP achieves the most favorable outcomes, with our method typically ranking closely behind. This demonstrates our method's effectiveness in balancing proximity and plausibility constraints. Notably, our method's computational time efficiency closely parallels the CBCE method, which does not involve an optimization process. This efficiency is due to our batching strategy, which processes all datasets collectively, as opposed to the case-by-case optimization typical of other methods. Summarizing, our method generates probabilistically plausible counterfactuals with exceptional efficiency and minimal compromise on proximity. Its ability to process high-dimensional data quickly makes it ideal for resource-constrained, real-world applications.

#### 6 Method Analysis

In this section, we delve into the analysis of two pivotal components of our proposed method: the loss function and the regularization hyperparameter λ. Adhering to the experimental framework established in the earlier sections, these studies are conducted specifically using the Logistic Regression model. Our focus is on evaluating the impact of these elements on the method's overall performance and efficacy.

#### *6.1 Loss Function Ablation Study*

In this ablation study, we examined the influence of discriminative loss function selection on the effectiveness of our proposed method. While Binary Cross Entropy (BCE) and Cross Entropy (CE) losses are conventional choices for binary and multiclass problems, respectively,

<span id="page-6-0"></span>Table 2: Analysis of Counterfactual Methods Across Classification Models. We offer a detailed comparison of our method and other wellestablished reference methods across two classification models: a 3-layer Multilayer Perceptron (MLP), and a Neural Oblivious Decision Ensemble (NODE). The results emphasize the efficacy of our method in producing valid and plausible counterfactuals across various models, including those that are deeper and more complex.

| DATASET | METHOD | COV. ↑ | VAL. ↑ | PROB. PLAUS. ↑ | LOF      | ISOFOREST | LOG DENS. ↑ | L1 ↓  | L2 ↓ | TIME ↓     |  |
|---------|--------|--------|--------|----------------|----------|-----------|-------------|-------|------|------------|--|
|         | MLP    |        |        |                |          |           |             |       |      |            |  |
|         | CBCE   | 1.00   | 0.94   | 0.54           | 1.09     | 0.08      | 28.85       | 2.87  | 0.82 | 6.47 S     |  |
|         | CEGP   | 0.94   | 0.63   | 0.05           | 4.15·108 | 0.01      | -3.28       | 1.25  | 0.43 | 31309.33 S |  |
| HELOC   | CEM    | 1.00   | 0.86   | 0.01           | 7.71·108 | -0.01     | -89.39      | 1.32  | 0.58 | 6938.45 S  |  |
|         | WACH   | 0.99   | 0.81   | 0.00           | 1.34·108 | -0.06     | -161.68     | 3.11  | 0.90 | 23392.40 S |  |
|         | ARTELT | -      | -      | -              | -        | -         | -           | -     | -    | - S        |  |
|         | PPCEF  | 1.00   | 0.92   | 1.00           | 1.42·108 | 0.07      | 32.07       | 1.18  | 0.31 | 25.32 S    |  |
|         | CBCE   | 1.00   | 1.00   | 0.18           | 1.02     | 0.04      | 23.66       | 16.29 | 3.09 | 0.54 S     |  |
|         | CEGP   | 0.95   | 0.46   | 0.02           | 1.24     | -0.02     | -138.62     | 6.39  | 1.42 | 2523.28 S  |  |
| DIGITS  | CEM    | 1.00   | 0.42   | 0.01           | 1.44     | -0.06     | -481.57     | 6.34  | 1.76 | 1260.54 S  |  |
|         | WACH   | 1.00   | 0.72   | 0.00           | 1.50     | -0.07     | -516.44     | 11.04 | 2.13 | 3342.38 S  |  |
|         | ARTELT | -      | -      | -              | -        | -         | -           | -     | -    | - S        |  |
|         | PPCEF  | 1.00   | 1.00   | 0.98           | 1.13     | 0.03      | 43.87       | 8.78  | 1.42 | 25.09 S    |  |
|         |        |        |        |                | NODE     |           |             |       |      |            |  |
|         | CBCE   | 1.00   | 1.00   | 0.55           | 1.09     | 0.08      | 28.88       | 2.85  | 0.82 | 17.53 S    |  |
|         | CEM    | 0.94   | 1.00   | 0.10           | 1.35     | 0.05      | 9.00        | 0.47  | 0.29 | 14772.66 S |  |
| HELOC   | WACH   | 0.96   | 1.00   | 0.10           | 2.12·108 | 0.05      | 10.75       | 0.85  | 0.36 | 37254.33 S |  |
|         | ARTELT | -      | -      | -              | -        | -         | -           | -     | -    | - S        |  |
|         | PPCEF  | 1.00   | 0.94   | 1.00           | 1.08     | 0.09      | 31.85       | 1.02  | 0.28 | 126.05 S   |  |
|         | CBCE   | 1.00   | 1.00   | 0.18           | 1.02     | 0.04      | 24.00       | 16.27 | 3.09 | 3.12 S     |  |
|         | CEM    | 1.00   | 1.00   | 0.03           | 1.32     | -0.02     | -39.458     | 4.07  | 1.44 | 5451.835 S |  |
| DIGITS  | WACH   | 1.00   | 1.00   | 0.16           | 1.12     | 0.02      | 7.02        | 2.93  | 1.13 | 15376.44 S |  |
|         | ARTELT | -      | -      | -              | -        | -         | -           | -     | -    | - S        |  |
|         | PPCEF  | 1.00   | 1.00   | 1.00           | 1.15     | 0.02      | 43.97       | 7.76  | 1.36 | 69.45 S    |  |

Table 3: Ablation Study on Loss Function Selection.

<span id="page-6-1"></span>

| DATASET | LOSS | COV. | VAL. | PP   | L1    | L2   | LD    |
|---------|------|------|------|------|-------|------|-------|
| MOONS   | OURS | 1.00 | 1.00 | 1.00 | 0.45  | 0.36 | 1.69  |
|         | BCE  | 1.00 | 1.00 | 0.99 | 0.89  | 0.69 | 1.74  |
| LAW     | OURS | 1.00 | 1.00 | 1.00 | 0.37  | 0.23 | 2.05  |
|         | BCE  | 1.00 | 1.00 | 0.98 | 0.97  | 0.60 | 1.67  |
| AUDIT   | OURS | 1.00 | 0.99 | 0.99 | 2.04  | 0.79 | 51.64 |
|         | BCE  | 1.00 | 0.99 | 0.98 | 3.01  | 1.25 | 52.54 |
| HELOC   | OURS | 1.00 | 0.99 | 0.99 | 0.85  | 0.23 | 37.50 |
|         | BCE  | 1.00 | 0.97 | 0.99 | 1.91  | 0.54 | 34.50 |
| BLOBS   | OURS | 1.00 | 1.00 | 1.00 | 0.69  | 0.50 | 3.00  |
|         | CE   | 1.00 | 1.00 | 0.93 | 0.82  | 0.60 | 2.85  |
| DIGITS  | OURS | 1.00 | 1.00 | 1.00 | 8.27  | 1.33 | 44.42 |
|         | CE   | 1.00 | 1.00 | 1.00 | 12.67 | 2.13 | 44.18 |
| WINE    | OURS | 1.00 | 1.00 | 1.00 | 1.65  | 0.53 | 9.72  |
|         | CE   | 1.00 | 1.00 | 0.99 | 3.87  | 1.29 | 9.29  |

we compared them against our proposed discriminative loss function to understand their impacts on the results. The findings, detailed in Tab. [3,](#page-6-1) reveal a notable distinction in distance metrics. Our method, using the specialized loss function, demonstrated significantly better proximity to original observations compared to BCE and CE. This improvement is attributed to our loss function's design, which zeroes the classification component of the loss upon surpassing by ϵ a classification threshold. This allows for more rapid convergence to closer counterfactuals, while CE, by continually seeking points with higher classification confidence, tends to push counterfactuals further from the original samples. Consequently, this affects the final values in proximity metrics, underscoring the advantage of our approach in generating more proximate and plausible counterfactuals.

# *6.2 Regularization Hyperparameter* λ *Analysis*

To evaluate the impact of the regularization hyperparameter λ on the fulfillment of validity and probabilistic plausibility conditions, we conducted a focused hyperparameter sensitivity analysis. While λ theoretically should extend to infinity, practical considerations neces-

<span id="page-6-2"></span>Table 4: Ablation Study on Regularization Hyperparameter λ.

| DATASET | λ    | COV. | VAL. | PP   | L1   | L2   | LD   |
|---------|------|------|------|------|------|------|------|
| MOONS   | 1    | 1.00 | 0.46 | 0.78 | 0.43 | 0.34 | 1.61 |
|         | 2    | 1.00 | 0.95 | 0.92 | 0.43 | 0.34 | 1.63 |
|         | 5    | 1.00 | 0.99 | 0.98 | 0.43 | 0.34 | 1.66 |
|         | 10   | 1.00 | 0.99 | 1.00 | 0.44 | 0.35 | 1.70 |
|         | 100  | 1.00 | 1.00 | 1.00 | 0.45 | 0.36 | 1.70 |
|         | 1000 | 1.00 | 1.00 | 1.00 | 0.45 | 0.36 | 1.70 |
| LAW     | 1    | 1.00 | 0.48 | 0.98 | 0.19 | 0.12 | 1.85 |
|         | 2    | 1.00 | 0.99 | 0.99 | 0.28 | 0.18 | 1.88 |
|         | 5    | 1.00 | 1.00 | 1.00 | 0.29 | 0.18 | 1.94 |
|         | 10   | 1.00 | 1.00 | 1.00 | 0.30 | 0.18 | 2.00 |
|         | 100  | 1.00 | 1.00 | 1.00 | 0.34 | 0.21 | 2.08 |
|         | 1000 | 1.00 | 1.00 | 1.00 | 0.38 | 0.22 | 2.09 |

sitate setting a feasible value. Our objective is to identify an optimal λ that not only guarantees condition fulfillment but also to understand its influence on other metrics. Experiments were carried out on the Moons and Law datasets, exploring λ values within the set {1, 2, 5, 10, 100, 1000}. The results in Tab. [4](#page-6-2) indicate that moderate values of λ, like 5 or 10, deliver satisfactory outcomes, while values around 100 or more almost invariably guarantee the fulfillment of the conditions, leading us to adopt the value of 100 for all preceding experiments. This experiment confirms the expected trade-off: higher strictness in counterfactual conditions leads to decreased proximity metrics, requiring larger deviations from the original data point.

# 7 Conclusions

In this work, we present PPCEF, a novel method for generating counterfactual explanations that utilize normalizing flows as density estimators within an unconstrained optimization framework. This technique adeptly balances essential factors such as distance, validity, and probabilistic plausibility in the counterfactuals it produces. Notably, PPCEF is computationally efficient and capable of handling large datasets, making it highly applicable in real-world scenarios. The method's flexible design allows for future enhancements, including other desirable counterfactual attributes like actionability or sparsity, and to generate plausible counterfactuals in label-scarce environments.

#### Acknowledgements

Patryk Wielopolski, Oleksii Furman, and Maciej Zieba's work was supported by the National Science Centre (Poland) Grant No. 2021/43/B/ST6/02853, and Jerzy Stefanowski's work was supported by the National Science Centre (Poland) grant No. 2023/51/B/ST6/00545. Moreover, we gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2023/016636.

#### References

- <span id="page-7-21"></span>[1] A. B. Arrieta and J. D. Ser. Plausible counterfactuals: Auditing deep learning classifiers with realistic adversarial examples. In *2020 International Joint Conference on Neural Networks, IJCNN 2020, Glasgow, United Kingdom, July 19-24, 2020*, pages 1–7. IEEE, 2020.
- <span id="page-7-6"></span>[2] A. Artelt and B. Hammer. On the computation of counterfactual explanations - A survey. *CoRR*, abs/1911.07749, 2019.
- <span id="page-7-10"></span>[3] A. Artelt and B. Hammer. Convex density constraints for computing plausible counterfactual explanations. In *Artificial Neural Networks and Machine Learning - ICANN 2020 - 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15-18, 2020, Proceedings, Part I*, volume 12396 of *Lecture Notes in Computer Science*, pages 353–365. Springer, 2020.
- <span id="page-7-23"></span>[4] M. Augustin, V. Boreiko, F. Croce, and M. Hein. Diffusion visual counterfactual explanations. In *Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022.
- <span id="page-7-14"></span>[5] A. Dhurandhar, P. Chen, R. Luss, C. Tu, P. Ting, K. Shanmugam, and P. Das. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. In *Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, pages 590–601, 2018.
- <span id="page-7-26"></span>[6] L. Dinh, D. Krueger, and Y. Bengio. NICE: non-linear independent components estimation. In *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings*, 2015.
- <span id="page-7-27"></span>[7] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real NVP. In *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*, 2017.
- <span id="page-7-24"></span>[8] A. Dombrowski, J. E. Gerken, K. Müller, and P. Kessel. Diffeomorphic counterfactuals with generative models. *CoRR*, abs/2206.05075, 2022. doi: 10.48550/ARXIV.2206.05075.
- <span id="page-7-25"></span>[9] T. D. Duong, Q. Li, and G. Xu. Ceflow: A robust and efficient counterfactual explanation framework for tabular data using normalizing flows. In *Advances in Knowledge Discovery and Data Mining - 27th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2023, Osaka, Japan, May 25-28, 2023, Proceedings, Part II*, volume 13936 of *Lecture Notes in Computer Science*, pages 133–144. Springer, 2023.
- <span id="page-7-5"></span>[10] R. Guidotti. Counterfactual explanations and how to find them: literature review and benchmarking. *Data Mining and Knowledge Discovery*, pages 1–55, 04 2022.
- <span id="page-7-22"></span>[11] G. Jeanneret, L. Simon, and F. Jurie. Diffusion models for counterfactual explanations. In *Computer Vision - ACCV 2022 - 16th Asian Conference on Computer Vision, Macao, China, December 4-8, 2022, Proceedings, Part VII*, volume 13847 of *Lecture Notes in Computer Science*, pages 219–237. Springer, 2022.
- <span id="page-7-13"></span>[12] J. Jiang, J. Lan, F. Leofante, A. Rago, and F. Toni. Provably robust and plausible counterfactual explanations for neural networks via robust optimisation. *CoRR*, abs/2309.12545, 2023.
- <span id="page-7-16"></span>[13] K. Kanamori, T. Takagi, K. Kobayashi, and H. Arimura. DACE: distribution-aware counterfactual explanation by mixed-integer linear optimization. In *Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020*, pages 2855–2862. ijcai.org, 2020.
- <span id="page-7-8"></span>[14] M. T. Keane and B. Smyth. Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable AI (XAI). In *Case-Based Reasoning Research and Development - 28th International Conference, ICCBR 2020, Salamanca, Spain, June 8-12, 2020, Proceedings*, volume 12311 of *Lecture Notes in Computer Science*, pages 163–178. Springer, 2020.
- <span id="page-7-18"></span>[15] A. V. Looveren and J. Klaise. Interpretable counterfactual explanations guided by prototypes. In *Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part II*, volume 12976 of *Lecture Notes in Computer Science*, pages 650–665. Springer, 2021.
- <span id="page-7-19"></span>[16] D. Mahajan, C. Tan, and A. Sharma. Preserving causal constraints in counterfactual explanations for machine learning classifiers. *CoRR*, abs/1912.03277, 2019.
- <span id="page-7-4"></span>[17] S. Mertes, T. Huber, K. Weitz, A. Heimerl, and E. André. Ganterfactual—counterfactual explanations for medical non-experts using generative adversarial learning. *Frontiers in Artificial Intelligence*, 2022.
- <span id="page-7-17"></span>[18] R. K. Mothilal, A. Sharma, and C. Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In *FAT\* '20: Conference on Fairness, Accountability, and Transparency, Barcelona, Spain, January 27-30, 2020*, pages 607–617. ACM, 2020.
- <span id="page-7-28"></span>[19] G. Papamakarios, I. Murray, and T. Pavlakou. Masked autoregressive flow for density estimation. In *Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pages 2338– 2347, 2017.
- <span id="page-7-33"></span>[20] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, highperformance deep learning library. In *Advances in Neural Information Processing Systems 32*, pages 8024–8035. Curran Associates, Inc., 2019.
- <span id="page-7-2"></span>[21] J. Pearl, M. Glymour, and N. Jewell. *Causal Inference in Statistics: A Primer*. Wiley, 2016. ISBN 9781119186847.
- <span id="page-7-29"></span>[22] S. Popov, S. Morozov, and A. Babenko. Neural oblivious decision ensembles for deep learning on tabular data. In *8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.
- <span id="page-7-12"></span>[23] R. Poyiadzi, K. Sokol, R. Santos-Rodríguez, T. D. Bie, and P. A. Flach. FACE: feasible and actionable counterfactual explanations. In *AIES '20: AAAI/ACM Conference on AI, Ethics, and Society, New York, NY, USA, February 7-8, 2020*, pages 344–350. ACM, 2020.
- <span id="page-7-11"></span>[24] D. Rezende and S. Mohamed. Variational inference with normalizing flows. In *International Conference on Machine Learning*, pages 1530– 1538. PMLR, 2015.
- <span id="page-7-15"></span>[25] C. Russell. Efficient search for diverse coherent explanations. In danah boyd and J. H. Morgenstern, editors, *Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT\* 2019, Atlanta, GA, USA, January 29-31, 2019*, pages 20–28. ACM, 2019.
- <span id="page-7-7"></span>[26] G. Shakhnarovich, T. Darrell, and P. Indyk. Nearest-neighbor methods in learning and vision. *IEEE Trans. Neural Networks*, 19(2):377, 2008.
- <span id="page-7-9"></span>[27] B. Smyth and M. T. Keane. A few good counterfactuals: Generating interpretable, plausible and diverse counterfactual explanations. In *Case-Based Reasoning Research and Development - 30th International Conference, ICCBR 2022, Nancy, France, September 12-15, 2022, Proceedings*, volume 13405 of *Lecture Notes in Computer Science*, pages 18–32. Springer, 2022.
- <span id="page-7-32"></span>[28] G. Van Rossum and F. L. Drake Jr. *Python reference manual*. Centrum voor Wiskunde en Informatica Amsterdam, 1995.
- <span id="page-7-20"></span>[29] N. Vercheval and A. Pizurica. Hierarchical variational autoencoders for visual counterfactuals. In *2021 IEEE International Conference on Image Processing, ICIP 2021, Anchorage, AK, USA, September 19-22, 2021*, pages 2513–2517. IEEE, 2021.
- <span id="page-7-0"></span>[30] S. Verma, V. Boonsanong, M. Hoang, K. E. Hines, J. P. Dickerson, and C. Shah. Counterfactual explanations and algorithmic recourses for machine learning: A review. *arXiv preprint arXiv:2010.10596*, 2020.
- <span id="page-7-1"></span>[31] S. Wachter, B. D. Mittelstadt, and C. Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. *CoRR*, abs/1711.00399, 2017.
- <span id="page-7-3"></span>[32] G. P. Wellawatte, A. Seshadri, and A. D. White. Model agnostic generation of counterfactual explanations for molecules. *Chem. Sci.*, 2022.
- <span id="page-7-30"></span>[33] P. Wielopolski, O. Furman, J. Stefanowski, and M. Zieba. Probabilistically plausible counterfactual explanations with normalizing flows. *CoRR*, abs/2405.17640, 2024. doi: 10.48550/ARXIV.2405.17640.
- <span id="page-7-31"></span>[34] L. F. Wightman. Lsac national longitudinal bar passage study. lsac research report series. Technical report, Law School Admission Council, Newtown, PA., 1998.

# <span id="page-8-0"></span>A Additional Results

This section supplements the main manuscript's experimental results with a more comprehensive analysis. We include means and standard deviations from five-fold cross-validation for greater statistical rigor and broaden the comparison with additional datasets and metrics. Tab. [5](#page-9-0) compares the effectiveness and nuances of different approaches in generating counterfactual explanations for the Logistic Regression model. In Tab. [6,](#page-10-0) we delve into Multilayer Perceptron, presenting a similar analysis that highlights the unique aspects and performance metrics relevant to this model. Moving forward, we examine the performance of counterfactual methods for deep ensembles of oblivious differentiable decision trees. Results for NODE classifier are presented in Tab. [7.](#page-11-0) Notably, Artelt's method is incompatible with non-linear classifiers such as Multilayer Perceptrons (MLP) or NODE, precluding the acquisition of performance data for these models using this approach. Furthermore, the application of CEGP to the NODE classifier was prohibitively time-consuming, which prevented the generation of results for this method as well. In Tab. [8,](#page-12-3) we provide extended results for the Ablation Study on Loss Function with additional metrics: LOF and Isolation Forest.

| DA<br>TAS<br>ET | ME<br>TH<br>OD                                                                 | CO<br>↑<br>VER<br>AG<br>E                                                                                                                          | VA<br>↑<br>LID<br>ITY                                                                                                                              | PRO<br>PLA<br>↑<br>B.<br>US                                                                                                                        | LO<br>F                                                                                                                                                                                                                                                                      | ISO<br>FO<br>RES<br>T                                                                                                                                  | LO<br>DE<br>↑<br>G<br>NS                                                                                                                                                      | L1<br>↓                                                                                                                                             | L2<br>↓                                                                                                                                                 | TIM<br>↓<br>E                                                                                                                                                                                                |
|-----------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| MO<br>ON<br>S   | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>PP<br>CE<br>F | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>8±<br>0.0<br>3<br>0±<br>1.0<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>1<br>0±<br>1.0<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 0.1<br>0±<br>0.2<br>3<br>9±<br>0.0<br>0.0<br>4<br>0.1<br>4±<br>0.0<br>3<br>0.1<br>1±<br>0.0<br>5<br>8±<br>0.0<br>0.0<br>3<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>6±<br>0.0<br>2<br>6±<br>1.3<br>0.0<br>3<br>2.0<br>3±<br>0.1<br>2<br>1.5<br>5±<br>0.0<br>8<br>3±<br>1.5<br>0.0<br>9<br>1.0<br>1±<br>0.0<br>2                                                                                                                           | 0.0<br>3±<br>0.0<br>0<br>0±<br>0.0<br>0.0<br>0<br>-0.<br>07±<br>0.0<br>1<br>-0.<br>01±<br>0.0<br>0<br>03±<br>-0.<br>0.0<br>1<br>0.0<br>4±<br>0.0<br>1  | -5.<br>81±<br>3.7<br>4<br>-6.<br>66±<br>0.8<br>2<br>-10<br>.09<br>±6<br>.62<br>-6.<br>34±<br>2.4<br>1<br>74±<br>-8.<br>3.5<br>7<br>1.6<br>9±<br>0.0<br>7                      | 0.6<br>2±<br>0.0<br>7<br>6±<br>0.3<br>0.0<br>2<br>0.5<br>5±<br>0.0<br>3<br>0.4<br>9±<br>0.0<br>2<br>2±<br>0.3<br>0.0<br>2<br>0.4<br>5±<br>0.0<br>1  | 0.4<br>8±<br>0.0<br>5<br>0.2<br>8±<br>0.0<br>1<br>0.5<br>0±<br>0.0<br>2<br>0.3<br>6±<br>0.0<br>1<br>2±<br>0.3<br>0.0<br>2<br>0.3<br>6±<br>0.0<br>1      | 0.0<br>7±<br>0.0<br>1<br>S<br>±1<br>904<br>.11<br>1.1<br>2<br>S<br>21<br>1.5<br>6±<br>1.5<br>0<br>S<br>198<br>.29<br>±3<br>.66<br>S<br>5±<br>4.1<br>0.6<br>9<br>S<br>1.8<br>5±<br>0.0<br>1<br>S              |
| LA<br>W         | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>CE<br>PP<br>F | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 0.4<br>9±<br>0.3<br>5<br>0.4<br>9±<br>0.0<br>4<br>0.2<br>6±<br>0.0<br>1<br>9±<br>0.3<br>0.0<br>3<br>0.4<br>0±<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>5±<br>0.0<br>2<br>1.0<br>7±<br>0.0<br>0<br>1.2<br>6±<br>0.0<br>0<br>0±<br>1.3<br>0.0<br>2<br>1.1<br>2±<br>0.0<br>1<br>1.0<br>3±<br>0.0<br>0                                                                                                                           | 0.0<br>4±<br>0.0<br>2<br>0.0<br>4±<br>0.0<br>0<br>-0.<br>02±<br>0.0<br>0<br>01±<br>-0.<br>0.0<br>0<br>0.0<br>2±<br>0.0<br>0<br>0.0<br>7±<br>0.0<br>0   | 1.2<br>8±<br>0.4<br>1<br>1.0<br>8±<br>0.0<br>4<br>-0.<br>56±<br>0.0<br>8<br>29±<br>-0.<br>0.1<br>3<br>0.5<br>4±<br>0.0<br>8<br>2.0<br>5±<br>0.0<br>2                          | 0.6<br>1±<br>0.0<br>3<br>0.2<br>3±<br>0.0<br>1<br>0.3<br>3±<br>0.0<br>1<br>5±<br>0.4<br>0.0<br>1<br>0.2<br>0±<br>0.0<br>1<br>0.3<br>7±<br>0.0<br>1  | 0.4<br>0±<br>0.0<br>2<br>0.1<br>8±<br>0.0<br>1<br>0.3<br>1±<br>0.0<br>1<br>5±<br>0.3<br>0.0<br>1<br>0.2<br>0±<br>0.0<br>1<br>0.2<br>3±<br>0.0<br>1      | 0.2<br>3±<br>0.0<br>0<br>S<br>197<br>3.7<br>6±<br>11.<br>09<br>S<br>368<br>.10<br>±5<br>1.5<br>7<br>S<br>359<br>±4<br>.00<br>1.3<br>7<br>S<br>4.0<br>2±<br>0.4<br>2<br>S<br>2.4<br>2±<br>0.1<br>0<br>S       |
| AU<br>DIT       | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>PP<br>CE<br>F | 1.0<br>0±<br>0.0<br>0<br>0.9<br>7±<br>0.0<br>2<br>0.5<br>2±<br>0.0<br>3<br>0.9<br>9±<br>0.0<br>1<br>0.6<br>0±<br>0.2<br>2<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>7±<br>0.0<br>5<br>0.9<br>9±<br>0.0<br>1 | 0.7<br>9±<br>0.2<br>8<br>0.0<br>2±<br>0.0<br>3<br>0.0<br>0±<br>0.0<br>1<br>0.0<br>2±<br>0.0<br>2<br>0.0<br>0±<br>0.0<br>1<br>0.9<br>9±<br>0.0<br>1 | 11.<br>70±<br>20.<br>10<br>7±<br>7<br>6.0<br>8·1<br>0<br>5.5<br>8·1<br>0<br>6±<br>7<br>8.2<br>8·1<br>0<br>1.8<br>5·1<br>0<br>8±<br>7<br>1.4<br>2·1<br>0<br>2.9<br>9·1<br>0<br>8±<br>8<br>4.0<br>9·1<br>0<br>5.8<br>9·1<br>0<br>7±<br>7<br>4.2<br>5·1<br>0<br>9.3<br>2·1<br>0 | 0.1<br>4±<br>0.0<br>0<br>0.0<br>2±<br>0.0<br>2<br>-0.<br>04±<br>0.0<br>3<br>0.0<br>6±<br>0.0<br>1<br>0.1<br>0±<br>0.0<br>2<br>0.0<br>8±<br>0.0<br>1    | 54.<br>97±<br>3.8<br>9<br>8.0<br>9±<br>13.<br>27<br>20.<br>84±<br>10.<br>32<br>-40<br>.34<br>±3<br>4.8<br>3<br>-35<br>85.<br>76±<br>783<br>4.2<br>9<br>51.<br>64±<br>4.5<br>3 | 2.5<br>5±<br>0.1<br>8<br>1.5<br>6±<br>0.1<br>3<br>1.2<br>0±<br>0.0<br>6<br>1.7<br>8±<br>0.0<br>8<br>0.9<br>0±<br>0.1<br>4<br>2.0<br>4±<br>0.1<br>5  | 1.2<br>4±<br>0.1<br>0<br>0.5<br>7±<br>0.0<br>6<br>0.3<br>7±<br>0.0<br>2<br>0.8<br>0±<br>0.0<br>5<br>0.8<br>8±<br>0.1<br>0<br>0.7<br>9±<br>0.1<br>2      | 0.0<br>4±<br>0.0<br>1<br>S<br>56<br>1.0<br>4±<br>13.<br>04<br>S<br>105<br>.92<br>±1<br>3.2<br>0<br>S<br>101<br>.27<br>±1<br>0.9<br>5<br>S<br>43.<br>84±<br>31.<br>22<br>S<br>7.0<br>1±<br>1.0<br>8<br>S      |
| HE<br>LO<br>C   | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>PP<br>CE<br>F | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0±<br>1.0<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>-<br>1.0<br>0±<br>0.0<br>0                     | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0±<br>1.0<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>-<br>1.0<br>0±<br>0.0<br>0                     | 0.5<br>4±<br>0.0<br>1<br>9±<br>0.2<br>0.0<br>3<br>7±<br>0.0<br>0.0<br>1<br>0.0<br>0±<br>0.0<br>0<br>-<br>1.0<br>0±<br>0.0<br>0                     | 0±<br>1.1<br>0.0<br>8<br>7±<br>6<br>3.5<br>0·1<br>0<br>7.2<br>8·1<br>0<br>7<br>8±<br>2.5<br>0·1<br>0<br>4.0<br>0·1<br>0<br>8±<br>7<br>2.6<br>5·1<br>0<br>3.0<br>4·1<br>0<br>-<br>7±<br>7<br>6.4<br>7·1<br>0<br>2.1<br>6·1<br>0                                               | 7±<br>0.0<br>0.0<br>3<br>4±<br>0.0<br>0.0<br>0<br>2±<br>0.0<br>0.0<br>1<br>0.0<br>3±<br>0.0<br>1<br>-<br>0.0<br>7±<br>0.0<br>0                         | 01±<br>28.<br>3.3<br>1<br>75±<br>0.5<br>24.<br>2<br>37±<br>12.<br>2.7<br>4<br>-15<br>.09<br>±5<br>.86<br>-<br>32.<br>42±<br>0.3<br>4                                          | 4±<br>2.8<br>0.3<br>9<br>0.2<br>6±<br>0.0<br>3<br>5±<br>0.3<br>0.0<br>5<br>0.7<br>4±<br>0.0<br>6<br>-<br>0.9<br>0±<br>0.0<br>3                      | 2±<br>0.8<br>0.1<br>1<br>0.1<br>0±<br>0.0<br>1<br>0±<br>0.2<br>0.0<br>2<br>0.3<br>7±<br>0.0<br>1<br>-<br>0.2<br>3±<br>0.0<br>1                          | 1±<br>0.4<br>1<br>5.7<br>S<br>965<br>0±<br>4.6<br>81.<br>96<br>S<br>6±<br>163<br>9.1<br>9.3<br>5<br>S<br>160<br>0.2<br>8±<br>17.<br>36<br>S<br>-<br>12.<br>44±<br>2.3<br>6<br>S                              |
| BL<br>OB<br>S   | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>CE<br>PP<br>F | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>6±<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 0.2<br>7±<br>0.1<br>5<br>0.0<br>0±<br>0.0<br>0<br>0.0<br>0±<br>0.0<br>0<br>4±<br>0.0<br>0.0<br>2<br>0.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>2±<br>0.0<br>3<br>2.4<br>3±<br>0.1<br>2<br>3.5<br>1±<br>0.0<br>9<br>4±<br>2.2<br>0.1<br>9<br>2.1<br>1±<br>0.1<br>6<br>1.0<br>1±<br>0.0<br>1                                                                                                                           | 0.0<br>3±<br>0.0<br>2<br>-0.<br>07±<br>0.0<br>1<br>-0.<br>12±<br>0.0<br>0<br>06±<br>-0.<br>0.0<br>1<br>-0.<br>07±<br>0.0<br>1<br>0.0<br>4±<br>0.0<br>1 | -35<br>.52<br>±1<br>5.6<br>8<br>-9.<br>08±<br>2.2<br>9<br>-14<br>.95<br>±2<br>.91<br>52±<br>-9.<br>1.8<br>0<br>-3.<br>51±<br>0.3<br>3<br>3.0<br>0±<br>0.1<br>1                | 0.9<br>5±<br>0.0<br>1<br>0.3<br>0±<br>0.0<br>1<br>0.4<br>6±<br>0.0<br>4<br>0.5<br>1±<br>0.0<br>2<br>0.3<br>9±<br>0.0<br>1<br>0.6<br>9±<br>0.0<br>5  | 0.7<br>2±<br>0.0<br>1<br>0.2<br>5±<br>0.0<br>1<br>0.4<br>5±<br>0.0<br>3<br>8±<br>0.3<br>0.0<br>2<br>0.3<br>3±<br>0.0<br>1<br>0.5<br>0±<br>0.0<br>4      | 0.1<br>3±<br>0.0<br>0<br>S<br>129<br>5.3<br>6±<br>30.<br>32<br>S<br>512<br>.56<br>±5<br>.56<br>S<br>1.5<br>9±<br>15.<br>44<br>39<br>S<br>6.6<br>2±<br>0.0<br>9<br>S<br>3.2<br>2±<br>0.8<br>4<br>S            |
| DIG<br>ITS      | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>CE<br>PP<br>F | 0±<br>1.0<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0.8<br>0±<br>0.1<br>2<br>1.0<br>0±<br>0.0<br>0 | 0±<br>1.0<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>8±<br>0.9<br>0.0<br>4<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>3±<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>0 | 8±<br>0.1<br>0.0<br>4<br>0.1<br>1±<br>0.0<br>3<br>1±<br>0.0<br>0.0<br>0<br>0.0<br>8±<br>0.0<br>3<br>0.0<br>4±<br>0.0<br>3<br>1.0<br>0±<br>0.0<br>0 | 2±<br>1.0<br>0.0<br>6<br>1.0<br>9±<br>0.0<br>1<br>3±<br>1.2<br>0.0<br>1<br>1.2<br>0±<br>0.0<br>1<br>1.6<br>9±<br>0.0<br>5<br>1.1<br>2±<br>0.0<br>1                                                                                                                           | 4±<br>0.0<br>0.0<br>0<br>0.0<br>1±<br>0.0<br>0<br>03±<br>-0.<br>0.0<br>1<br>0.0<br>0±<br>0.0<br>1<br>0.0<br>1±<br>0.0<br>1<br>0.0<br>3±<br>0.0<br>1    | 72±<br>23.<br>4.7<br>3<br>-0.<br>39±<br>4.8<br>0<br>-86<br>±1<br>.77<br>7.2<br>4<br>-34<br>.97<br>±7<br>.01<br>-54<br>.72<br>±1<br>1.8<br>1<br>44.<br>42±<br>1.8<br>7         | 28±<br>16.<br>0.6<br>2<br>2.5<br>3±<br>0.1<br>1<br>5.2<br>8±<br>4.9<br>3<br>2.4<br>7±<br>0.0<br>5<br>3.3<br>0±<br>0.1<br>3<br>8.2<br>7±<br>0.2<br>4 | 9±<br>3.0<br>0.0<br>2<br>0.6<br>3±<br>0.0<br>2<br>8±<br>5<br>1.3<br>0.7<br>1.2<br>0±<br>0.0<br>2<br>2.4<br>3±<br>0.1<br>4<br>1.3<br>3±<br>0.0<br>4      | 1±<br>0.5<br>0.1<br>6<br>S<br>194<br>5.6<br>7±<br>22.<br>30<br>S<br>852<br>.05<br>±2<br>7.6<br>8<br>S<br>65<br>1.0<br>0±<br>7.9<br>7<br>S<br>238<br>.28<br>±3<br>3.2<br>8<br>S<br>8.6<br>8±<br>3.6<br>5<br>S |
| WI<br>NE        | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT<br>PP<br>CE<br>F | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>7±<br>0.0<br>4<br>1.0<br>0±<br>0.0<br>0 | 0.3<br>7±<br>0.0<br>5<br>0.0<br>1±<br>0.0<br>1<br>0±<br>0.0<br>0.0<br>0<br>0.0<br>1±<br>0.0<br>1<br>0.0<br>1±<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>0 | 1.0<br>6±<br>0.0<br>5<br>1.0<br>8±<br>0.0<br>3<br>5±<br>1.3<br>0.0<br>3<br>1.2<br>7±<br>0.0<br>4<br>1.3<br>3±<br>0.0<br>3<br>1.0<br>1±<br>0.0<br>1                                                                                                                           | 0.0<br>5±<br>0.0<br>2<br>0.0<br>5±<br>0.0<br>1<br>02±<br>-0.<br>0.0<br>1<br>0.0<br>0±<br>0.0<br>1<br>0.0<br>2±<br>0.0<br>0<br>0.0<br>9±<br>0.0<br>1    | 2.1<br>3±<br>1.2<br>8<br>-0.<br>15±<br>1.5<br>4<br>±1<br>-12<br>.94<br>.94<br>-9.<br>41±<br>1.9<br>5<br>-11<br>.73<br>±2<br>.36<br>9.7<br>2±<br>0.6<br>2                      | 3.3<br>8±<br>0.1<br>4<br>0.8<br>2±<br>0.0<br>8<br>0±<br>1.2<br>0.0<br>9<br>1.5<br>7±<br>0.1<br>0<br>0.6<br>5±<br>0.0<br>5<br>1.6<br>5±<br>0.0<br>9  | 1.1<br>2±<br>0.0<br>5<br>0.3<br>2±<br>0.0<br>3<br>0.6<br>3±<br>5<br>0.0<br>0.7<br>8±<br>0.0<br>5<br>0.9<br>6±<br>0.0<br>8<br>S<br>0.5<br>3±<br>0.0<br>4 | 0.0<br>1±<br>0.0<br>0<br>S<br>191<br>.09<br>±4<br>.00<br>S<br>33±<br>81.<br>1.8<br>8<br>S<br>50.<br>74±<br>5.6<br>3<br>S<br>2.0<br>3±<br>0.4<br>7<br>S                                                       |

<span id="page-9-0"></span>Table 5: Detailed Comparative Results of Probabilistically Plausible Counterfactual Explanation Methods for Logistic Regression classifier. We present a comprehensive comparison of our method with other established reference methods across various datasets. The results presented include the mean and standard deviation obtained from a five-fold cross-validation.

<span id="page-10-0"></span>Table 6: Detailed Comparative Results of Probabilistically Plausible Counterfactual Explanation Methods for Multilayer Perceptron classifier. We provide an in-depth comparison between our approach and other well-established methods, utilizing a range of datasets. This analysis includes both the average values and the standard deviations derived from five-fold cross-validation.

| DA<br>TAS<br>ET | ME<br>TH<br>OD                                                | CO<br>↑<br>VER<br>AG<br>E                                                                             | VA<br>↑<br>LID<br>ITY                                                                                 | PRO<br>PLA<br>↑<br>B.<br>US                                                                           | LO<br>F                                                                                                                                                                         | ISO<br>FO<br>RES<br>T                                                                                    | LO<br>DE<br>↑<br>G<br>NS                                                                                                       | L1<br>↓                                                                                                 | L2<br>↓                                                                                               | TIM<br>↓<br>E                                                                                                                                                        |
|-----------------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| MO<br>ON<br>S   | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH                   | 1.0<br>0±<br>0.0<br>0<br>0.9<br>7±<br>0.0<br>3<br>9±<br>0.9<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>1      | 0.8<br>4±<br>0.2<br>4<br>0.3<br>3±<br>0.1<br>3<br>5±<br>5<br>0.4<br>0.1<br>0.5<br>6±<br>0.0<br>6      | 0.5<br>8±<br>0.1<br>6<br>0.0<br>6±<br>0.0<br>7<br>3±<br>0.0<br>0.0<br>4<br>0.0<br>4±<br>0.0<br>5      | 1.0<br>3±<br>0.0<br>3<br>1.3<br>9±<br>0.1<br>0<br>9±<br>6<br>2.2<br>0.1<br>1.5<br>2±<br>0.1<br>2                                                                                | 0.0<br>2±<br>0.0<br>1<br>0.0<br>0±<br>0.0<br>1<br>08±<br>-0.<br>0.0<br>1<br>-0.<br>01±<br>0.0<br>1       | 1.2<br>3±<br>0.2<br>7<br>-3.<br>67±<br>1.1<br>6<br>±6<br>-11<br>.92<br>.48<br>-3.<br>71±<br>2.5<br>4                           | 0.7<br>1±<br>0.1<br>6<br>0.2<br>9±<br>0.0<br>7<br>9±<br>0.4<br>0.0<br>4<br>0.2<br>8±<br>0.0<br>9        | 0.5<br>3±<br>0.1<br>1<br>0.2<br>3±<br>0.0<br>5<br>8±<br>0.4<br>0.0<br>4<br>0.2<br>4±<br>0.0<br>8      | 0.0<br>8±<br>0.0<br>0<br>S<br>156<br>2.3<br>4±<br>119<br>.65<br>S<br>±3<br>784<br>.34<br>3.8<br>3<br>S<br>145<br>2.0<br>1±<br>99.<br>08<br>S                         |
|                 | AR<br>TE<br>LT<br>CE<br>PP<br>F                               | -<br>1.0<br>0±<br>0.0<br>0                                                                            | -<br>0.9<br>8±<br>0.0<br>1                                                                            | -<br>1.0<br>0±<br>0.0<br>0                                                                            | -<br>0.9<br>9±<br>0.0<br>2                                                                                                                                                      | -<br>0.0<br>3±<br>0.0<br>1                                                                               | -<br>1.6<br>2±<br>0.0<br>4                                                                                                     | -<br>0.4<br>4±<br>0.0<br>5                                                                              | -<br>0.3<br>4±<br>0.0<br>4                                                                            | S<br>-<br>20.<br>44±<br>1.7<br>5<br>S                                                                                                                                |
| LA<br>W         | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | 1.0<br>0±<br>0.0<br>0<br>0.9<br>3±<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>- | 0.7<br>9±<br>0.1<br>1<br>0.2<br>8±<br>0.0<br>1<br>0.6<br>1±<br>0.0<br>4<br>0.7<br>4±<br>0.0<br>4<br>- | 0.3<br>6±<br>0.4<br>0<br>0.5<br>3±<br>0.0<br>5<br>7±<br>0.2<br>0.0<br>1<br>0.4<br>2±<br>0.0<br>3<br>- | 1.0<br>5±<br>0.0<br>2<br>1.0<br>7±<br>0.0<br>0<br>6±<br>1.2<br>0.0<br>1<br>1.1<br>4±<br>0.0<br>1<br>-                                                                           | 0.0<br>3±<br>0.0<br>3<br>0.0<br>4±<br>0.0<br>0<br>02±<br>-0.<br>0.0<br>0<br>0.0<br>1±<br>0.0<br>0<br>-   | 1.1<br>8±<br>0.4<br>5<br>1.2<br>3±<br>0.1<br>6<br>22±<br>-0.<br>0.1<br>1<br>0.5<br>8±<br>0.0<br>9<br>-                         | 0.6<br>7±<br>0.0<br>8<br>0.2<br>4±<br>0.0<br>2<br>9±<br>0.2<br>0.0<br>1<br>0.3<br>8±<br>0.0<br>1<br>-   | 0.4<br>4±<br>0.0<br>5<br>0.1<br>7±<br>0.0<br>1<br>8±<br>0.2<br>0.0<br>1<br>0.2<br>9±<br>0.0<br>1<br>- | 0.2<br>8±<br>0.0<br>2<br>S<br>298<br>6.4<br>4±<br>60.<br>71<br>S<br>3±<br>141<br>3.0<br>97.<br>23<br>S<br>245<br>9.8<br>8±<br>78.<br>30<br>S<br>S<br>-               |
|                 | PP<br>CE<br>F                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 0.9<br>5±<br>0.0<br>1                                                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 3±<br>1.0<br>0.0<br>0                                                                                                                                                           | 7±<br>0.0<br>0.0<br>0                                                                                    | 2.0<br>4±<br>0.0<br>2                                                                                                          | 0±<br>0.4<br>0.0<br>1                                                                                   | 4±<br>0.2<br>0.0<br>1                                                                                 | 63±<br>20.<br>1.0<br>8<br>S                                                                                                                                          |
| AU<br>DIT       | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | 1.0<br>0±<br>0.0<br>0<br>0.5<br>5±<br>0.0<br>2<br>0.5<br>3±<br>0.0<br>2<br>0.4<br>9±<br>0.0<br>2<br>- | 0.9<br>3±<br>0.0<br>7<br>0.4<br>8±<br>0.0<br>2<br>0.5<br>4±<br>0.0<br>5<br>0.7<br>8±<br>0.0<br>6<br>- | 0.6<br>1±<br>0.2<br>1<br>0.0<br>2±<br>0.0<br>3<br>0.0<br>1±<br>0.0<br>1<br>0.0<br>0±<br>0.0<br>0<br>- | 11.<br>10±<br>13.<br>30<br>2±<br>1<br>1.4<br>5·1<br>0<br>3.8<br>6·1<br>0<br>7±<br>7<br>6.5<br>2·1<br>0<br>8.7<br>6·1<br>0<br>2±<br>1<br>1.2<br>4·1<br>0<br>2.7<br>5·1<br>0<br>- | 0.1<br>4±<br>0.0<br>0<br>0.0<br>0±<br>0.0<br>3<br>0.0<br>4±<br>0.0<br>3<br>-0.<br>03±<br>0.0<br>3<br>-   | 55.<br>02±<br>3.4<br>9<br>-85<br>.64<br>±4<br>1.7<br>7<br>-13<br>2.4<br>3±<br>98.<br>13<br>-15<br>5.2<br>0±<br>74.<br>93<br>-  | 2.7<br>7±<br>0.2<br>5<br>1.4<br>2±<br>0.1<br>2<br>1.1<br>9±<br>0.1<br>4<br>1.4<br>9±<br>0.0<br>7<br>-   | 1.3<br>4±<br>0.1<br>4<br>0.5<br>7±<br>0.0<br>3<br>0.6<br>1±<br>0.0<br>4<br>0.6<br>5±<br>0.0<br>1<br>- | 0.1<br>6±<br>0.0<br>1<br>S<br>142<br>6.3<br>3±<br>55.<br>75<br>S<br>44<br>1.8<br>5±<br>7.5<br>6<br>S<br>60<br>1.7<br>7±<br>23.<br>34<br>S<br>S<br>-                  |
|                 | CE<br>PP<br>F                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 0.9<br>9±<br>0.0<br>2                                                                                 | 0.9<br>9±<br>0.0<br>1                                                                                 | 7±<br>7<br>1.4<br>4·1<br>0<br>1.8<br>8·1<br>0                                                                                                                                   | 0.0<br>8±<br>0.0<br>1                                                                                    | 51.<br>72±<br>4.5<br>9                                                                                                         | 2.1<br>4±<br>0.1<br>1                                                                                   | 0.8<br>3±<br>0.1<br>0                                                                                 | 46.<br>60±<br>3.8<br>2<br>S                                                                                                                                          |
| HE<br>LO<br>C   | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | 1.0<br>0±<br>0.0<br>0<br>0.9<br>4±<br>0.0<br>1<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>9±<br>0.0<br>0<br>- | 0.9<br>4±<br>0.0<br>1<br>0.6<br>3±<br>0.0<br>1<br>0.8<br>6±<br>0.0<br>1<br>0.8<br>1±<br>0.0<br>3<br>- | 0.5<br>4±<br>0.0<br>3<br>0.0<br>5±<br>0.0<br>1<br>0.0<br>1±<br>0.0<br>0<br>0.0<br>0±<br>0.0<br>0<br>- | 9±<br>1.0<br>0.0<br>8<br>8±<br>8<br>4.1<br>5·1<br>0<br>1.9<br>7·1<br>0<br>8±<br>8<br>7.7<br>1·1<br>0<br>1.4<br>1·1<br>0<br>8±<br>7<br>1.3<br>4·1<br>0<br>4.4<br>4·1<br>0<br>-   | 8±<br>0.0<br>0.0<br>3<br>0.0<br>1±<br>0.0<br>0<br>-0.<br>01±<br>0.0<br>1<br>-0.<br>06±<br>0.0<br>1<br>-  | 85±<br>5<br>28.<br>3.9<br>-3.<br>28±<br>4.7<br>9<br>-89<br>.39<br>±5<br>6.8<br>9<br>-16<br>1.6<br>8±<br>104<br>.43<br>-        | 7±<br>2.8<br>0.4<br>0<br>1.2<br>5±<br>0.1<br>1<br>1.3<br>2±<br>0.2<br>2<br>3.1<br>1±<br>0.2<br>4<br>-   | 2±<br>0.8<br>0.1<br>1<br>0.4<br>3±<br>0.0<br>3<br>0.5<br>8±<br>0.0<br>7<br>0.9<br>0±<br>0.0<br>4<br>- | 6.4<br>7±<br>0.6<br>9<br>S<br>313<br>09.<br>33±<br>334<br>2.4<br>0<br>S<br>693<br>8.4<br>5±<br>79.<br>08<br>S<br>233<br>92.<br>40±<br>367<br>7.1<br>4<br>S<br>S<br>- |
|                 | PP<br>CE<br>F                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 0.9<br>2±<br>0.0<br>3                                                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 8±<br>7<br>1.4<br>2·1<br>0<br>3.9<br>0·1<br>0                                                                                                                                   | 0.0<br>7±<br>0.0<br>0                                                                                    | 32.<br>07±<br>0.6<br>3                                                                                                         | 1.1<br>8±<br>0.1<br>1                                                                                   | 0.3<br>1±<br>0.0<br>3                                                                                 | 25.<br>32±<br>6.4<br>1<br>S                                                                                                                                          |
| BL<br>OB<br>S   | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | 1.0<br>0±<br>0.0<br>0<br>0.9<br>8±<br>0.0<br>2<br>0.9<br>7±<br>0.0<br>4<br>0.9<br>9±<br>0.0<br>1<br>- | 1.0<br>0±<br>0.0<br>0<br>0.4<br>7±<br>0.0<br>4<br>0.7<br>1±<br>0.0<br>7<br>0.5<br>7±<br>0.0<br>4<br>- | 0.2<br>7±<br>0.1<br>5<br>0.0<br>0±<br>0.0<br>0<br>0.0<br>0±<br>0.0<br>0<br>0.0<br>0±<br>0.0<br>0<br>- | 1.0<br>2±<br>0.0<br>3<br>2.4<br>8±<br>0.1<br>6<br>3.3<br>6±<br>0.1<br>2<br>2.6<br>7±<br>0.1<br>5<br>-                                                                           | 0.0<br>3±<br>0.0<br>2<br>-0.<br>07±<br>0.0<br>0<br>-0.<br>11±<br>0.0<br>0<br>-0.<br>07±<br>0.0<br>1<br>- | -35<br>.45<br>±1<br>5.4<br>8<br>-9.<br>54±<br>3.3<br>0<br>-18<br>.58<br>±1<br>0.3<br>9<br>-10<br>.72<br>±3<br>.68<br>-         | 0.9<br>5±<br>0.0<br>1<br>0.3<br>5±<br>0.0<br>1<br>0.4<br>6±<br>0.0<br>2<br>0.3<br>4±<br>0.0<br>1<br>-   | 0.7<br>2±<br>0.0<br>1<br>0.2<br>9±<br>0.0<br>1<br>0.4<br>5±<br>0.0<br>1<br>0.2<br>9±<br>0.0<br>1<br>- | 0.1<br>6±<br>0.0<br>0<br>S<br>304<br>7.7<br>9±<br>38.<br>09<br>S<br>104<br>6.3<br>5±<br>14.<br>92<br>S<br>173<br>1.1<br>0±<br>62.<br>81<br>S<br>S<br>-               |
|                 | PP<br>CE<br>F                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 1.0<br>3±<br>0.0<br>2                                                                                                                                                           | 0.0<br>4±<br>0.0<br>0                                                                                    | 2.9<br>1±<br>0.0<br>4                                                                                                          | 0.6<br>5±<br>0.0<br>1                                                                                   | 0.4<br>7±<br>0.0<br>1                                                                                 | 19.<br>55±<br>0.3<br>0<br>S                                                                                                                                          |
| DIG<br>ITS      | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | 1.0<br>0±<br>0.0<br>0<br>5±<br>0.9<br>0.0<br>1<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>- | 1.0<br>0±<br>0.0<br>0<br>6±<br>0.4<br>0.0<br>3<br>0.4<br>2±<br>0.0<br>1<br>0.7<br>2±<br>0.0<br>3<br>- | 0.1<br>8±<br>0.0<br>4<br>2±<br>0.0<br>0.0<br>1<br>0.0<br>1±<br>0.0<br>1<br>0.0<br>0±<br>0.0<br>0<br>- | 1.0<br>2±<br>0.0<br>6<br>4±<br>1.2<br>0.0<br>0<br>1.4<br>4±<br>0.0<br>1<br>1.5<br>0±<br>0.0<br>3<br>-                                                                           | 0.0<br>4±<br>0.0<br>0<br>03±<br>-0.<br>0.0<br>0<br>-0.<br>06±<br>0.0<br>1<br>-0.<br>07±<br>0.0<br>1<br>- | 23.<br>66±<br>3.7<br>6<br>2±<br>-13<br>8.6<br>14.<br>93<br>-48<br>1.5<br>7±<br>68.<br>49<br>-51<br>6.4<br>4±<br>72.<br>51<br>- | 16.<br>29±<br>0.6<br>1<br>9±<br>6.3<br>0.1<br>6<br>6.3<br>4±<br>0.5<br>5<br>11.<br>04±<br>0.3<br>6<br>- | 3.0<br>9±<br>0.0<br>2<br>1.4<br>2±<br>0.0<br>3<br>1.7<br>6±<br>0.0<br>9<br>2.1<br>3±<br>0.0<br>7<br>- | 0.5<br>4±<br>0.1<br>6<br>S<br>252<br>8±<br>54.<br>3.2<br>03<br>S<br>126<br>0.5<br>4±<br>51.<br>03<br>S<br>334<br>2.3<br>8±<br>104<br>.14<br>S<br>S<br>-              |
|                 | PP<br>CE<br>F                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 0.9<br>8±<br>0.0<br>1                                                                                 | 1.1<br>3±<br>0.0<br>1                                                                                                                                                           | 0.0<br>3±<br>0.0<br>1                                                                                    | 43.<br>87±<br>2.3<br>8                                                                                                         | 8.7<br>8±<br>0.2<br>9                                                                                   | 1.4<br>2±<br>0.0<br>5                                                                                 | 25.<br>09±<br>0.4<br>0<br>S                                                                                                                                          |
| WI<br>NE        | CB<br>CE<br>CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | 1.0<br>0±<br>0.0<br>0<br>8±<br>0.9<br>0.0<br>2<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>- | 0.9<br>6±<br>0.0<br>4<br>2±<br>0.4<br>0.0<br>5<br>0.4<br>4±<br>0.0<br>6<br>0±<br>6<br>0.8<br>0.1<br>- | 0.3<br>7±<br>0.0<br>5<br>6±<br>0.0<br>0.0<br>8<br>0.0<br>0±<br>0.0<br>0<br>1±<br>0.0<br>0.0<br>1<br>- | 1.1<br>1±<br>0.0<br>3<br>2±<br>1.1<br>0.0<br>3<br>1.4<br>0±<br>0.0<br>5<br>9±<br>1.2<br>0.1<br>0<br>-                                                                           | 0.0<br>3±<br>0.0<br>2<br>4±<br>0.0<br>0.0<br>1<br>-0.<br>02±<br>0.0<br>1<br>0±<br>0.0<br>0.0<br>2<br>-   | 1.7<br>1±<br>1.9<br>3<br>36±<br>-1.<br>3.4<br>0<br>-17<br>.90<br>±4<br>.72<br>.69<br>±7<br>-10<br>.38<br>-                     | 4.0<br>5±<br>0.2<br>2<br>1±<br>1.3<br>0.1<br>8<br>1.2<br>4±<br>0.0<br>9<br>7±<br>2.0<br>0.2<br>3<br>-   | 1.3<br>1±<br>0.0<br>6<br>8±<br>0.4<br>0.0<br>7<br>0.6<br>7±<br>0.0<br>6<br>0±<br>6<br>0.8<br>0.0<br>- | 0.0<br>1±<br>0.0<br>0<br>S<br>±4<br>486<br>.70<br>6.6<br>0<br>S<br>117<br>.67<br>±3<br>.86<br>S<br>.63<br>±6<br>.95<br>224<br>S<br>S<br>-                            |
|                 | PP<br>CE<br>F                                                 | 1.0<br>0±<br>0.0<br>0                                                                                 | 0.9<br>7±<br>0.0<br>3                                                                                 | 0.9<br>9±<br>0.0<br>1                                                                                 | 1.0<br>1±<br>0.0<br>1                                                                                                                                                           | 0.0<br>9±<br>0.0<br>1                                                                                    | 9.7<br>9±<br>0.5<br>9                                                                                                          | 1.7<br>1±<br>0.1<br>0                                                                                   | 0.5<br>5±<br>0.0<br>4                                                                                 | 10.<br>32±<br>0.7<br>3<br>S                                                                                                                                          |

| DA<br>TAS<br>ET | ME<br>TH<br>OD                                    | CO<br>↑<br>VER<br>AG<br>E                                | VA<br>↑<br>LID<br>ITY                                    | PRO<br>PLA<br>↑<br>B.<br>US                              | LO<br>F                                                                                                  | ISO<br>FO<br>RES<br>T                                     | LO<br>DE<br>↑<br>G<br>NS                                          | L1<br>↓                                                  | L2<br>↓                                                  | TIM<br>↓<br>E                                                                                 |
|-----------------|---------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|-------------------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|-----------------------------------------------------------------------------------------------|
|                 | CB<br>CE                                          | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 0.5<br>0±<br>0.0<br>1                                    | 1.0<br>4±<br>0.0<br>3                                                                                    | 0.0<br>2±<br>0.0<br>1                                     | 0.9<br>0±<br>1.1<br>1                                             | 0.6<br>2±<br>0.0<br>6                                    | 0.4<br>8±<br>0.0<br>4                                    | 0.4<br>1±<br>0.0<br>9<br>S                                                                    |
| MO<br>ON<br>S   | CE<br>GP<br>CE<br>M<br>WA<br>CH                   | -<br>0.9<br>9±<br>0.0<br>1<br>1.0<br>0±<br>0.0<br>0      | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0      | -<br>7±<br>0.0<br>0.0<br>2<br>0.0<br>2±<br>0.0<br>2      | -<br>0±<br>2.0<br>0.0<br>9<br>1.2<br>9±<br>0.0<br>4                                                      | -<br>07±<br>-0.<br>0.0<br>1<br>0.0<br>1±<br>0.0<br>0      | -<br>36±<br>-4.<br>2.7<br>9<br>-0.<br>48±<br>0.3<br>3             | -<br>0.6<br>1±<br>0.0<br>3<br>0.3<br>3±<br>0.0<br>1      | -<br>0.5<br>7±<br>0.0<br>2<br>0.2<br>6±<br>0.0<br>1      | S<br>-<br>1.6<br>8±<br>15.<br>101<br>44<br>S<br>173<br>6.5<br>4±<br>19.<br>44<br>S            |
|                 | AR<br>TE<br>LT<br>CE<br>PP<br>F                   | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>0.9<br>9±<br>0.0<br>0                               | -<br>0.9<br>9±<br>0.0<br>1                                                                               | -<br>0.0<br>3±<br>0.0<br>1                                | -<br>1.6<br>0±<br>0.0<br>3                                        | -<br>0.4<br>2±<br>0.0<br>3                               | -<br>0.3<br>3±<br>0.0<br>3                               | S<br>-<br>39.<br>68±<br>4.4<br>8<br>S                                                         |
|                 | CB<br>CE                                          | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 0.4<br>8±<br>0.3<br>6                                    | 1.0<br>5±<br>0.0<br>2                                                                                    | 0.0<br>4±<br>0.0<br>2                                     | 1.2<br>7±<br>0.4<br>3                                             | 0.6<br>1±<br>0.0<br>3                                    | 0.4<br>0±<br>0.0<br>2                                    | 1.0<br>8±<br>0.0<br>4<br>S                                                                    |
| LA<br>W         | CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | -<br>0.9<br>7±<br>0.0<br>1<br>0.9<br>9±<br>0.0<br>1<br>- | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>- | -<br>0.2<br>9±<br>0.0<br>1<br>0.4<br>5±<br>0.0<br>6<br>- | -<br>1.2<br>7±<br>0.0<br>2<br>1.0<br>8±<br>0.0<br>1<br>-                                                 | -<br>-0.<br>02±<br>0.0<br>0<br>0.0<br>3±<br>0.0<br>0<br>- | -<br>-0.<br>41±<br>0.1<br>4<br>1.1<br>6±<br>0.0<br>6<br>-         | -<br>0.3<br>3±<br>0.0<br>2<br>0.4<br>1±<br>0.0<br>1<br>- | -<br>0.3<br>1±<br>0.0<br>1<br>0.2<br>9±<br>0.0<br>0<br>- | S<br>-<br>210<br>3.3<br>0±<br>31.<br>69<br>S<br>440<br>1.6<br>0±<br>240<br>.03<br>S<br>S<br>- |
|                 | PP<br>CE<br>F                                     | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>3±<br>0.0<br>0                                                                                    | 0.0<br>7±<br>0.0<br>0                                     | 2.0<br>4±<br>0.0<br>1                                             | 0.3<br>8±<br>0.0<br>1                                    | 0.2<br>3±<br>0.0<br>1                                    | 61.<br>67±<br>2.8<br>8<br>S                                                                   |
|                 | CB<br>CE<br>CE<br>GP                              | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 9±<br>0.7<br>0.2<br>8                                    | 80±<br>11.<br>20.<br>40                                                                                  | 4±<br>0.1<br>0.0<br>0                                     | 54.<br>92±<br>3.9<br>2                                            | 2.5<br>5±<br>0.1<br>9                                    | 4±<br>1.2<br>0.1<br>0                                    | 0.3<br>8±<br>0.0<br>8<br>S                                                                    |
| AU<br>DIT       | CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT             | -<br>0.5<br>2±<br>0.0<br>3<br>0.9<br>8±<br>0.0<br>3<br>- | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0<br>- | -<br>0.0<br>0±<br>0.0<br>1<br>0.0<br>3±<br>0.0<br>4<br>- | -<br>8±<br>8<br>1.5<br>2·1<br>0<br>1.4<br>3·1<br>0<br>7±<br>7<br>6.9<br>0·1<br>0<br>7.9<br>4·1<br>0<br>- | -<br>0.1<br>1±<br>0.0<br>0<br>0.0<br>6±<br>0.0<br>1<br>-  | -<br>12.<br>63±<br>14.<br>72<br>-33<br>.50<br>±5<br>0.9<br>5<br>- | -<br>1.0<br>6±<br>0.1<br>0<br>1.5<br>9±<br>0.1<br>4<br>- | -<br>0.5<br>6±<br>0.0<br>4<br>0.9<br>7±<br>0.0<br>4<br>- | S<br>-<br>906<br>.63<br>±2<br>4.0<br>7<br>S<br>234<br>7.9<br>1±<br>227<br>.09<br>S<br>S<br>-  |
|                 | PP<br>CE<br>F                                     | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 0.9<br>9±<br>0.0<br>1                                    | 7±<br>7<br>2.3<br>2·1<br>0<br>2.9<br>9·1<br>0                                                            | 0.0<br>9±<br>0.0<br>1                                     | 51.<br>67±<br>4.5<br>3                                            | 2.0<br>6±<br>0.1<br>2                                    | 0.8<br>0±<br>0.0<br>9                                    | 80.<br>34±<br>21.<br>47<br>S                                                                  |
|                 | CB<br>CE                                          | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 0.5<br>5±<br>0.0<br>3                                    | 1.0<br>9±<br>0.0<br>8                                                                                    | 0.0<br>8±<br>0.0<br>3                                     | 28.<br>88±<br>4.0<br>6                                            | 2.8<br>5±<br>0.4<br>0                                    | 0.8<br>2±<br>0.1<br>1                                    | 17.<br>53±<br>0.9<br>2<br>S                                                                   |
| HE<br>LO<br>C   | CE<br>GP<br>CE<br>M<br>WA<br>CH                   | -<br>0.9<br>4±<br>0.0<br>1<br>0.9<br>6±<br>0.0<br>3      | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0      | -<br>0.1<br>0±<br>0.0<br>1<br>0.1<br>0±<br>0.0<br>2      | -<br>1.3<br>5±<br>0.0<br>1<br>8±<br>8<br>2.1<br>2·1<br>0<br>4.2<br>3·1<br>0                              | -<br>0.0<br>5±<br>0.0<br>1<br>0.0<br>5±<br>0.0<br>1       | -<br>9.0<br>0±<br>3.6<br>1<br>10.<br>75±<br>10.<br>46             | -<br>0.4<br>7±<br>0.0<br>8<br>0.8<br>5±<br>0.0<br>5      | -<br>0.2<br>9±<br>0.0<br>2<br>0.3<br>6±<br>0.0<br>4      | S<br>-<br>147<br>72.<br>66±<br>226<br>.75<br>S<br>372<br>54.<br>33±<br>366<br>6.8<br>7<br>S   |
|                 | AR<br>TE<br>LT<br>PP<br>CE<br>F                   | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>4±<br>0.9<br>0.0<br>1                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>8±<br>1.0<br>0.0<br>0                                                                               | -<br>9±<br>0.0<br>0.0<br>0                                | -<br>31.<br>85±<br>0.4<br>1                                       | -<br>2±<br>6<br>1.0<br>0.0                               | -<br>8±<br>0.2<br>0.0<br>2                               | S<br>-<br>126<br>.05<br>±3<br>3.1<br>0<br>S                                                   |
|                 | CB<br>CE                                          | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 0.2<br>7±<br>0.1<br>5                                    | 1.0<br>2±<br>0.0<br>3                                                                                    | 0.0<br>3±<br>0.0<br>2                                     | -35<br>.52<br>±1<br>5.6<br>8                                      | 0.9<br>5±<br>0.0<br>1                                    | 0.7<br>2±<br>0.0<br>1                                    | 0.7<br>7±<br>0.0<br>8<br>S                                                                    |
| BL<br>OB<br>S   | CE<br>GP<br>CE<br>M<br>WA<br>CH<br>AR<br>TE<br>LT | -<br>0.9<br>0±<br>0.0<br>4<br>0.9<br>4±<br>0.0<br>2      | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0      | -<br>0.0<br>0±<br>0.0<br>0<br>0.0<br>0±<br>0.0<br>0      | -<br>3.1<br>0±<br>0.0<br>9<br>2.6<br>5±<br>0.1<br>1                                                      | -<br>-0.<br>11±<br>0.0<br>0<br>-0.<br>08±<br>0.0<br>0     | -<br>-24<br>.98<br>±1<br>0.4<br>2<br>-11<br>.76<br>±3<br>.58      | -<br>0.5<br>8±<br>0.0<br>2<br>0.3<br>7±<br>0.0<br>2      | -<br>0.5<br>2±<br>0.0<br>2<br>0.3<br>3±<br>0.0<br>1      | S<br>-<br>132<br>9.9<br>1±<br>14.<br>61<br>S<br>240<br>6.4<br>0±<br>59.<br>59<br>S<br>S       |
|                 | CE<br>PP<br>F                                     | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>2±<br>0.0<br>2                                                                               | -<br>0.0<br>4±<br>0.0<br>0                                | -<br>2.9<br>4±<br>0.0<br>4                                        | -<br>0.6<br>5±<br>0.0<br>1                               | -<br>0.4<br>7±<br>0.0<br>1                               | -<br>47.<br>69±<br>5.2<br>1<br>S                                                              |
|                 | CB<br>CE                                          | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 0.1<br>8±<br>0.0<br>4                                    | 1.0<br>2±<br>0.0<br>6                                                                                    | 0.0<br>4±<br>0.0<br>0                                     | 24.<br>00±<br>4.1<br>4                                            | 16.<br>27±<br>0.6<br>5                                   | 3.0<br>9±<br>0.0<br>3                                    | 3.1<br>2±<br>0.2<br>4<br>S                                                                    |
| DIG<br>ITS      | CE<br>GP<br>CE<br>M<br>WA<br>CH                   | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0      | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0      | -<br>0.0<br>3±<br>0.0<br>1<br>6±<br>0.1<br>0.0<br>3      | -<br>1.3<br>2±<br>0.0<br>1<br>2±<br>1.1<br>0.0<br>1                                                      | -<br>-0.<br>02±<br>0.0<br>0<br>2±<br>0.0<br>0.0<br>1      | -<br>-39<br>.45<br>±1<br>0.7<br>8<br>2±<br>5.0<br>7.0<br>0        | -<br>4.0<br>7±<br>0.1<br>4<br>2.9<br>3±<br>0.0<br>5      | -<br>1.4<br>4±<br>0.0<br>3<br>1.1<br>3±<br>0.0<br>1      | S<br>-<br>545<br>1.8<br>3±<br>19.<br>15<br>S<br>153<br>44±<br>76.<br>290<br>.44<br>S          |
|                 | AR<br>TE<br>LT<br>PP<br>CE<br>F                   | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.1<br>5±<br>0.0<br>1                                                                               | -<br>0.0<br>2±<br>0.0<br>1                                | -<br>43.<br>97±<br>1.9<br>5                                       | -<br>7.7<br>6±<br>0.2<br>0                               | -<br>1.3<br>6±<br>0.0<br>3                               | S<br>-<br>69.<br>45±<br>3.9<br>4<br>S                                                         |
|                 | CB<br>CE                                          | 1.0<br>0±<br>0.0<br>0                                    | 1.0<br>0±<br>0.0<br>0                                    | 9±<br>0.3<br>0.0<br>3                                    | 1±<br>1.1<br>0.0<br>4                                                                                    | 2±<br>0.0<br>0.0<br>2                                     | 4±<br>6<br>2.0<br>1.8                                             | 5±<br>4.0<br>0.2<br>2                                    | 1±<br>6<br>1.3<br>0.0                                    | 0.1<br>0±<br>0.0<br>7<br>S                                                                    |
| WI<br>NE        | CE<br>GP<br>CE<br>M<br>WA<br>CH                   | -<br>1.0<br>0±<br>0.0<br>0<br>0.9<br>9±<br>0.0<br>1      | -<br>1.0<br>0±<br>0.0<br>0<br>1.0<br>0±<br>0.0<br>0      | -<br>0.0<br>0±<br>0.0<br>0<br>0.0<br>8±<br>0.0<br>6      | -<br>1.3<br>4±<br>0.0<br>4<br>1.1<br>2±<br>0.0<br>4                                                      | -<br>0.0<br>0±<br>0.0<br>1<br>0.0<br>4±<br>0.0<br>1       | -<br>-21<br>.02<br>±1<br>4.0<br>0<br>-1.<br>05±<br>2.4<br>9       | -<br>1.1<br>1±<br>0.1<br>9<br>1.1<br>0±<br>0.0<br>4      | -<br>0.6<br>5±<br>0.0<br>6<br>0.5<br>9±<br>0.0<br>2      | S<br>-<br>225<br>.63<br>±1<br>1.1<br>8<br>S<br>459<br>.91<br>±1<br>3.2<br>2<br>S              |
|                 | AR<br>TE<br>LT<br>PP<br>CE<br>F                   | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>0±<br>0.0<br>0                               | -<br>1.0<br>1±<br>0.0<br>1                                                                               | -<br>0.0<br>9±<br>0.0<br>0                                | -<br>9.8<br>2±<br>0.6<br>6                                        | -<br>1.6<br>9±<br>0.0<br>9                               | -<br>0.5<br>5±<br>0.0<br>3                               | S<br>-<br>19.<br>39±<br>1.0<br>6<br>S                                                         |

<span id="page-11-0"></span>Table 7: Detailed Comparative Results of Probabilistically Plausible Counterfactual Explanation Methods for Neural Oblivious Decision Ensembles classifier. We offer a detailed comparison of our method with other established approaches using various datasets. This evaluation presents both the mean values and the standard deviations obtained through five-fold cross-validation.

<span id="page-12-3"></span>

| DATASET | LOSS | COV.↑     | VAL.↑     | PROB. PLAUS.↑ | LOF               | ISOFOREST  | LOG DENS.↑ | L1↓        | L2↓       |
|---------|------|-----------|-----------|---------------|-------------------|------------|------------|------------|-----------|
| MOONS   | OURS | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 1.01±0.02         | 0.04±0.01  | 1.69±0.07  | 0.45±0.01  | 0.36±0.01 |
|         | BCE  | 1.00±0.00 | 1.00±0.00 | 0.99±0.00     | 1.04±0.03         | -0.01±0.01 | 1.74±0.09  | 0.89±0.03  | 0.69±0.02 |
| LAW     | OURS | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 1.03±0.00         | 0.07±0.00  | 2.05±0.02  | 0.37±0.01  | 0.23±0.01 |
|         | BCE  | 1.00±0.00 | 1.00±0.00 | 0.98±0.01     | 0.99±0.00         | 0.01±0.01  | 1.67±0.01  | 0.97±0.02  | 0.60±0.01 |
| AUDIT   | OURS | 1.00±0.00 | 0.99±0.01 | 0.99±0.01     | 4.25·107±9.32·107 | 0.08±0.01  | 51.64±4.53 | 2.04±0.15  | 0.79±0.12 |
|         | BCE  | 1.00±0.00 | 0.99±0.01 | 0.98±0.01     | 3.59·108±1.16·108 | 0.09±0.01  | 52.54±4.54 | 3.01±0.20  | 1.25±0.10 |
| HELOC   | OURS | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 6.47·107±2.16·107 | 0.07±0.00  | 32.42±0.34 | 0.90±0.03  | 0.23±0.01 |
|         | BCE  | 1.00±0.00 | 1.00±0.00 | 0.99±0.00     | 1.67·108±6.18·107 | 0.05±0.01  | 32.11±0.45 | 2.77±0.13  | 0.78±0.05 |
| BLOBS   | OURS | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 1.01±0.01         | 0.04±0.01  | 3.00±0.11  | 0.69±0.05  | 0.50±0.04 |
|         | CE   | 1.00±0.00 | 1.00±0.00 | 0.93±0.01     | 1.05±0.02         | 0.03±0.01  | 2.85±0.04  | 0.82±0.02  | 0.60±0.01 |
| DIGITS  | OURS | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 1.12±0.01         | 0.03±0.01  | 44.42±1.87 | 8.27±0.24  | 1.33±0.04 |
|         | CE   | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 1.12±0.01         | 0.02±0.01  | 44.18±2.09 | 12.67±0.15 | 2.13±0.04 |
| WINE    | OURS | 1.00±0.00 | 1.00±0.00 | 1.00±0.00     | 1.01±0.01         | 0.09±0.01  | 9.72±0.62  | 1.65±0.09  | 0.53±0.04 |
|         | CE   | 1.00±0.00 | 1.00±0.00 | 0.99±0.01     | 1.06±0.03         | 0.02±0.01  | 9.29±0.71  | 3.87±0.18  | 1.29±0.06 |

Table 8: Detailed Ablation Study on Loss Function Selection. This table includes additional metrics: LOF and IsoForest

<span id="page-12-2"></span>Table 9: Dataset Characteristics and Model Performances. This table provides an overview of the datasets used in our experiments, including the number of samples (N), number of features (d), number of classes (C), accuracy of Logistic Regression (LR Acc.), Multi-Layer Perceptron (MLP Acc.), and the log density of the Masked Autoregressive Flow (MAF Log Dens.).

| DATASET | N      | d  | C  | LR ACC. | MLP ACC. | MAF LOG DENS. |
|---------|--------|----|----|---------|----------|---------------|
| MOONS   | 1,024  | 2  | 2  | 0.85    | 0.98     | 1.38          |
| LAW     | 2,220  | 3  | 2  | 0.75    | 0.75     | 1.23          |
| AUDIT   | 610    | 23 | 2  | 0.95    | 0.98     | 48.15         |
| HELOC   | 10,459 | 23 | 2  | 0.70    | 0.70     | 28.67         |
| WINE    | 178    | 13 | 3  | 0.90    | 0.97     | 7.21          |
| BLOBS   | 1,500  | 2  | 3  | 1.00    | 1.00     | 2.58          |
| DIGITS  | 5,620  | 64 | 10 | 0.94    | 0.95     | 35.80         |

#### <span id="page-12-0"></span>B Datasets

In Tab. [9,](#page-12-2) we provide detailed descriptions of the datasets utilized in our study: Moons, Law[2](#page-12-4) , Audit[3](#page-12-5) , Heloc[4](#page-12-6) , Wine[5](#page-12-7) , Blobs and Digits[6](#page-12-8) . The Moons dataset is an artificially generated set comprising two interleaving half-circles. It includes a standard deviation of Gaussian noise set at 0.01. The Law dataset originates from the Law School Admissions Council (LSAC) and is referred to in the literature as the Law School Admissions dataset [\[34\]](#page-7-31). For our analysis, we selected three features that exhibit the highest correlation with the target variable: entrance exam scores (LSAT), grade-point average (GPA), and first-year average grade (FYA). The Audit dataset, which encompasses comprehensive one-year non-confidential data of firms in the years 2015 to 2016, is collected from the Auditor Office of India to build a predictor for classifying suspicious firms. The Heloc dataset, initially utilized in the 'FICO xML Challenge', consists of Home Equity Line of Credit (HELOC) applications submitted by real homeowners. This dataset comprises various numeric features that encapsulate information from the applicant's credit report. The primary objective is to predict whether the applicant will repay their HELOC account within a two-year period. This prediction is instrumental in determining the applicant's qualification for a line of credit. The Wine dataset comprises chemical analysis results for wines originating from the same region in Italy, produced from three distinct cultivars. This analysis quantified 13 different constituents present in each of the three wine varieties. The Blobs dataset is an artificially generated isotropic Gaussian blobs, characterized by equal variance. The Digits dataset is utilized for the optical recognition of handwritten digits. It consists of 32x32 bitmap images that are segmented into non-overlapping 4x4 blocks. Within each block, the count of 'on' pixels is recorded, resulting in an 8x8 input matrix. Each element of this matrix is an integer between 0 and 16.

### <span id="page-12-1"></span>C Density Estimator - Additional Results

This experiment assesses the efficacy of Normalizing Flow models, particularly in high-dimensional datasets, against traditional Kernel Density Estimation (KDE). We compared KDE with three Normalizing Flow architectures: RealNVP, NICE, and Masked Autoregressive Flow (MAF). The mean log density results for the test datasets are detailed in Tab. [10.](#page-13-0) For lower-dimensional datasets like Moons, Law and Blobs, KDE and MAF show comparable performance, significantly outperforming RealNVP and NICE. However, in high-dimensional datasets such as Audit, Heloc and Digits, MAF demonstrates a substantial advantage over the other density estimators. These findings reinforce our proposition to employ Normalizing Flows, especially the MAF architecture, as effective density estimators in our method.

<span id="page-12-4"></span><sup>2</sup> <https://www.kaggle.com/datasets/danofer/law-school-admissions-bar-passage>

<span id="page-12-5"></span><sup>3</sup> <https://archive.ics.uci.edu/dataset/475/audit+data>

<span id="page-12-6"></span><sup>4</sup> <https://community.fico.com/s/explainable-machine-learning-challenge>

<span id="page-12-7"></span><sup>5</sup> <https://archive.ics.uci.edu/dataset/109/wine>

<span id="page-12-8"></span><sup>6</sup> <https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits>

<span id="page-13-0"></span>Table 10: Comparative Analysis of Density Estimators. We present the mean log density results for KDE, RealNVP, NICE, and MAF across various datasets. It highlights the superior performance of MAF in high-dimensional datasets (Audit and Heloc) and its comparable efficacy to KDE in lower-dimensional datasets (Moons and Law), underscoring the effectiveness of MAF as a density estimator in our method.

| 0.95±0.01<br>-1.85±0.00<br>-1.86±0.00<br>1.38±0.07<br>MOONS<br>1.16±0.03<br>-2.79±0.00<br>-2.80±0.00<br>1.23±0.05<br>LAW<br>10.75±28.15<br>-21.25±0.13<br>-21.34±0.22<br>48.15±8.41<br>AUDIT    | NICE<br>MAF               | REALNVP     | KDE        | DATASET |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|-------------|------------|---------|
| WINE<br>5.95±0.57<br>-12.05±0.02<br>-12.08±0.01<br>7.21±0.80<br>BLOBS<br>2.10±0.02<br>-1.84±0.00<br>-1.84±0.00<br>2.58±0.05<br>DIGITS<br>22.66±1.14<br>-59.33±0.05<br>-59.79±0.09<br>35.80±3.30 | -21.19±0.00<br>28.67±0.42 | -21.12±0.00 | 22.44±0.32 | HELOC   |

# D Implementation details

In the implementation of our experiments, we utilized Python [\[28\]](#page-7-32) as the primary programming language. The core of our computational framework was PyTorch [\[20\]](#page-7-33), a popular open-source machine learning library. A key feature of our implementation was the gradient optimization approach, designed to be executed in batches. This approach was particularly effective, allowing us to process entire test sets in a single batch. Our experiments were conducted on an M1 Apple Silicon CPU paired with 16GB of RAM. This hardware setup was more than sufficient for our experiment needs, providing enough capacity for the computational demands of our algorithm while ensuring fast processing speeds.